import { z } from 'zod';
import { ensureBinaryExists, logError, logInfo, logSuccess, logWarn, run } from './utils.js';
import yargs from 'yargs';
import { hideBin } from 'yargs/helpers';

const Args = z.object({
  action: z.enum(['install', 'uninstall', 'expand']),
  namespace: z.string().default('percona'),
  name: z.string().default('pxc-cluster'),
  helmRepo: z.string().default('https://percona.github.io/percona-helm-charts/'),
  chart: z.string().default('percona/pxc-operator'),
  clusterChart: z.string().default('percona/pxc-db'),
  nodes: z.coerce.number().int().positive().default(3),
  size: z.string().optional(),
});

type Args = z.infer<typeof Args>;

async function ensurePrereqs() {
  await ensureBinaryExists('kubectl', ['version', '--client']);
  await ensureBinaryExists('helm', ['version']);
}

async function validateEksCluster(ns: string) {
  logInfo('=== Validating EKS cluster state for Percona installation ===');
  const { execa } = await import('execa');
  
  let validationErrors: string[] = [];
  let validationWarnings: string[] = [];
  
  try {
    // 1. Check kubectl connectivity
    logInfo('Checking kubectl connectivity...');
    await run('kubectl', ['cluster-info'], { stdio: 'pipe' });
    logSuccess('âœ“ kubectl connectivity verified');
  } catch (error) {
    validationErrors.push('Cannot connect to Kubernetes cluster via kubectl');
  }
  
  try {
    // 2. Check cluster version and compatibility
    logInfo('Checking Kubernetes version...');
    
    // Try different kubectl version command formats
    let versionResult;
    let versionOutput = '';
    
    try {
      // Try newer kubectl format first
      versionResult = await execa('kubectl', ['version', '--output=yaml'], { stdio: 'pipe' });
      versionOutput = versionResult.stdout;
      logInfo(`Kubectl version output (yaml): ${versionOutput}`);
    } catch (yamlError) {
      try {
        // Try older format
        versionResult = await execa('kubectl', ['version', '--short'], { stdio: 'pipe' });
        versionOutput = versionResult.stdout;
        logInfo(`Kubectl version output (short): ${versionOutput}`);
      } catch (shortError) {
        // Try without any flags
        versionResult = await execa('kubectl', ['version'], { stdio: 'pipe' });
        versionOutput = versionResult.stdout;
        logInfo(`Kubectl version output (default): ${versionOutput}`);
      }
    }
    
    // Try multiple patterns to match version
    let versionMatch = versionOutput.match(/Server Version: v(\d+\.\d+)/);
    if (!versionMatch) {
      versionMatch = versionOutput.match(/serverVersion:\s+gitVersion:\s+v(\d+\.\d+)/);
    }
    if (!versionMatch) {
      versionMatch = versionOutput.match(/v(\d+\.\d+)/);
    }
    if (!versionMatch) {
      // Try parsing the full version string
      const fullVersionMatch = versionOutput.match(/Server Version: v(\d+\.\d+\.\d+)/);
      if (fullVersionMatch) {
        versionMatch = [fullVersionMatch[0], fullVersionMatch[1].split('.').slice(0, 2).join('.')];
      }
    }
    if (!versionMatch) {
      // Try YAML format
      const yamlVersionMatch = versionOutput.match(/serverVersion:\s+gitVersion:\s+v(\d+\.\d+\.\d+)/);
      if (yamlVersionMatch) {
        versionMatch = [yamlVersionMatch[0], yamlVersionMatch[1].split('.').slice(0, 2).join('.')];
      }
    }
    
    if (versionMatch) {
      const majorMinor = versionMatch[1];
      const [major, minor] = majorMinor.split('.').map(Number);
      
      logInfo(`Detected Kubernetes version: ${majorMinor} (major: ${major}, minor: ${minor})`);
      
      // Percona Operator requires Kubernetes 1.24+ (based on compatibility matrix)
      if (major < 1 || (major === 1 && minor < 24)) {
        validationErrors.push(`Kubernetes version ${majorMinor} is too old. Percona Operator requires 1.24+`);
      } else {
        logSuccess(`âœ“ Kubernetes version ${majorMinor} is compatible`);
      }
    } else {
      logWarn(`Could not parse Kubernetes version from: ${versionResult.stdout}`);
      
      // Fallback: try to get version from cluster info
      try {
        logInfo('Trying fallback method to get Kubernetes version...');
        const clusterInfoResult = await execa('kubectl', ['cluster-info'], { stdio: 'pipe' });
        logInfo(`Cluster info output: ${clusterInfoResult.stdout}`);
        
        // Look for version in cluster info
        const clusterVersionMatch = clusterInfoResult.stdout.match(/v(\d+\.\d+)/);
        if (clusterVersionMatch) {
          const majorMinor = clusterVersionMatch[1];
          const [major, minor] = majorMinor.split('.').map(Number);
          
          logInfo(`Detected Kubernetes version (fallback): ${majorMinor} (major: ${major}, minor: ${minor})`);
          
          if (major < 1 || (major === 1 && minor < 24)) {
            validationErrors.push(`Kubernetes version ${majorMinor} is too old. Percona Operator requires 1.24+`);
          } else {
            logSuccess(`âœ“ Kubernetes version ${majorMinor} is compatible (detected via fallback)`);
          }
        } else {
          validationWarnings.push('Could not determine Kubernetes version from kubectl output or cluster info');
        }
      } catch (fallbackError) {
        logWarn(`Fallback version check also failed: ${fallbackError}`);
        validationWarnings.push('Could not determine Kubernetes version from any method');
      }
    }
  } catch (error) {
    logWarn(`Error running kubectl version: ${error}`);
    validationWarnings.push(`Could not check Kubernetes version: ${error.message || error}`);
  }
  
  try {
    // 3. Check available nodes and resources
    logInfo('Checking cluster nodes and resources...');
    const nodesResult = await execa('kubectl', ['get', 'nodes', '-o', 'json'], { stdio: 'pipe' });
    const nodesData = JSON.parse(nodesResult.stdout);
    
    if (!nodesData.items || nodesData.items.length === 0) {
      validationErrors.push('No nodes found in the cluster');
    } else {
      const readyNodes = nodesData.items.filter((node: any) => 
        node.status.conditions?.some((c: any) => c.type === 'Ready' && c.status === 'True')
      );
      
      logInfo(`Found ${readyNodes.length}/${nodesData.items.length} ready nodes`);
      
      if (readyNodes.length < 3) {
        validationWarnings.push(`Only ${readyNodes.length} ready nodes found. Percona recommends at least 3 nodes for high availability`);
      }
      
      // Check node resources
      let totalCpu = 0;
      let totalMemory = 0;
      
      readyNodes.forEach((node: any) => {
        const cpu = node.status.allocatable?.['cpu'] || '0';
        const memory = node.status.allocatable?.['memory'] || '0';
        
        // Debug: log the actual values
        logInfo(`Node ${node.metadata.name}: CPU=${cpu}, Memory=${memory}`);
        
        // Convert CPU (e.g., "2" or "2000m" to millicores)
        const cpuMillicores = cpu.endsWith('m') ? parseInt(cpu) : parseInt(cpu) * 1000;
        totalCpu += cpuMillicores;
        
        // Convert memory (e.g., "8Gi" to bytes) - handle more formats
        let memoryBytes = 0;
        if (memory.endsWith('Gi')) {
          memoryBytes = parseInt(memory) * 1024 * 1024 * 1024;
        } else if (memory.endsWith('Mi')) {
          memoryBytes = parseInt(memory) * 1024 * 1024;
        } else if (memory.endsWith('Ki')) {
          memoryBytes = parseInt(memory) * 1024;
        } else if (memory.endsWith('G')) {
          memoryBytes = parseInt(memory) * 1000 * 1000 * 1000;
        } else if (memory.endsWith('M')) {
          memoryBytes = parseInt(memory) * 1000 * 1000;
        } else if (memory.endsWith('K')) {
          memoryBytes = parseInt(memory) * 1000;
        } else {
          // Try to parse as raw bytes
          memoryBytes = parseInt(memory) || 0;
        }
        
        totalMemory += memoryBytes;
        logInfo(`  Converted: CPU=${cpuMillicores}mc, Memory=${memoryBytes} bytes (${Math.round(memoryBytes/1024/1024/1024)}GB)`);
      });
      
      // Percona needs at least 3 CPU cores and 6GB RAM total
      const minCpu = 3000; // 3 cores in millicores
      const minMemory = 6 * 1024 * 1024 * 1024; // 6GB in bytes
      
      if (totalCpu < minCpu) {
        validationWarnings.push(`Total CPU capacity (${Math.round(totalCpu/1000)} cores) may be insufficient for 3-node Percona cluster`);
      }
      
      if (totalMemory < minMemory) {
        validationWarnings.push(`Total memory capacity (${Math.round(totalMemory/1024/1024/1024)}GB) may be insufficient for 3-node Percona cluster`);
      }
      
              logSuccess(`âœ“ Cluster has ${readyNodes.length} nodes with ${Math.round(totalCpu/1000)} CPU cores and ${Math.round(totalMemory/1024/1024/1024)}GB memory`);
              
              // Check for multi-AZ deployment
              const nodeZones = new Set();
              logInfo('=== AZ Detection Debug ===');
              logInfo(`Checking ${readyNodes.length} ready nodes for availability zone distribution...`);
              
              readyNodes.forEach((node: any, index: number) => {
                const zone = node.metadata.labels?.['topology.kubernetes.io/zone'] || 
                            node.metadata.labels?.['failure-domain.beta.kubernetes.io/zone'];
                logInfo(`Node ${index + 1}: ${node.metadata.name}`);
                logInfo(`  - topology.kubernetes.io/zone: ${node.metadata.labels?.['topology.kubernetes.io/zone'] || 'not found'}`);
                logInfo(`  - failure-domain.beta.kubernetes.io/zone: ${node.metadata.labels?.['failure-domain.beta.kubernetes.io/zone'] || 'not found'}`);
                logInfo(`  - Detected zone: ${zone || 'none'}`);
                
                if (zone) {
                  nodeZones.add(zone);
                  logInfo(`  âœ“ Added to zones set: ${zone}`);
                } else {
                  logInfo(`  âš ï¸  No zone label found for this node`);
                }
              });
              
              logInfo(`=== AZ Detection Results ===`);
              logInfo(`Total unique zones detected: ${nodeZones.size}`);
              logInfo(`Zones found: [${Array.from(nodeZones).join(', ')}]`);
              
              if (nodeZones.size >= 2) {
                logSuccess(`âœ“ Multi-AZ deployment detected: ${nodeZones.size} availability zones (${Array.from(nodeZones).join(', ')})`);
              } else if (nodeZones.size === 1) {
                validationErrors.push(`âŒ FATAL: Single AZ deployment detected (${Array.from(nodeZones)[0]}). Percona requires multi-AZ deployment for high availability. Please recreate your EKS cluster with nodes across multiple availability zones.`);
              } else {
                validationErrors.push('âŒ FATAL: Could not determine availability zones for nodes. Multi-AZ deployment is required for high availability.');
              }
            }
          } catch (error) {
            validationErrors.push(`Failed to check cluster nodes: ${error}`);
          }
  
  try {
    // 4. Check storage classes
    logInfo('Checking storage classes...');
    const scResult = await execa('kubectl', ['get', 'storageclass', '-o', 'json'], { stdio: 'pipe' });
    const scData = JSON.parse(scResult.stdout);
    
    const ebsCsiSc = scData.items?.find((sc: any) => 
      sc.provisioner === 'ebs.csi.aws.com' && sc.metadata.name === 'gp3'
    );
    
    if (!ebsCsiSc) {
      validationWarnings.push('gp3 storage class with EBS CSI driver not found. Will create one during installation.');
    } else {
      logSuccess('âœ“ gp3 storage class with EBS CSI driver found');
    }
    
    // Check if EBS CSI driver is running
    try {
      const ebsCsiResult = await execa('kubectl', ['get', 'pods', '-n', 'kube-system', '-l', 'app=ebs-csi-controller', '--no-headers'], { stdio: 'pipe' });
      const ebsCsiPods = ebsCsiResult.stdout.trim().split('\n').filter(line => line.trim());
      
      if (ebsCsiPods.length === 0) {
        validationWarnings.push('EBS CSI driver pods not found. Storage provisioning may fail.');
      } else {
        const readyPods = ebsCsiPods.filter(line => line.includes('Running'));
        if (readyPods.length === 0) {
          validationWarnings.push('EBS CSI driver pods are not running. Storage provisioning may fail.');
        } else {
          logSuccess('âœ“ EBS CSI driver is running');
        }
      }
    } catch (error) {
      validationWarnings.push('Could not check EBS CSI driver status');
    }
  } catch (error) {
    validationWarnings.push(`Failed to check storage classes: ${error}`);
  }
  
  // DNS resolution test moved to after namespace creation
  
  try {
    // 6. Check IAM roles and permissions
    logInfo('Checking IAM roles and permissions...');
    
    // First create the namespace if it doesn't exist
    try {
      await run('kubectl', ['create', 'namespace', ns], { stdio: 'pipe' });
      logInfo(`Created namespace ${ns} for validation`);
    } catch (error) {
      if (error.toString().includes('already exists')) {
        logInfo(`Namespace ${ns} already exists`);
      } else {
        validationWarnings.push(`Could not create namespace ${ns}: ${error}`);
      }
    }
    
    // Check DNS resolution now that namespace exists
    try {
      logInfo('Checking DNS resolution...');
      const dnsTestPod = `apiVersion: v1
kind: Pod
metadata:
  name: dns-test
  namespace: ${ns}
spec:
  containers:
  - name: dns-test
    image: busybox:1.35
    command: ['nslookup', 'kubernetes.default.svc.cluster.local']
  restartPolicy: Never`;
      
      const proc = execa('kubectl', ['apply', '-f', '-'], { stdio: ['pipe', 'pipe', 'pipe'] });
      proc.stdin?.write(dnsTestPod);
      proc.stdin?.end();
      await proc;
      
      // Wait for pod to complete
      await new Promise(resolve => setTimeout(resolve, 10000));
      
      try {
        const dnsResult = await execa('kubectl', ['logs', 'dns-test', '-n', ns], { stdio: 'pipe' });
        if (dnsResult.stdout.includes('kubernetes.default.svc.cluster.local')) {
          logSuccess('âœ“ DNS resolution working');
        } else {
          validationWarnings.push('DNS resolution may have issues');
        }
      } catch (error) {
        validationWarnings.push('Could not verify DNS resolution');
      } finally {
        // Clean up test pod
        try {
          await run('kubectl', ['delete', 'pod', 'dns-test', '-n', ns], { stdio: 'pipe' });
        } catch (error) {
          // Ignore cleanup errors
        }
      }
    } catch (error) {
      validationWarnings.push(`Failed to test DNS resolution: ${error}`);
    }
    
    // Check if we can create secrets (needed for S3 credentials)
    try {
      const testSecret = `apiVersion: v1
kind: Secret
metadata:
  name: test-secret
  namespace: ${ns}
type: Opaque
data:
  test: dGVzdA==`;
      
      const proc = execa('kubectl', ['apply', '-f', '-'], { stdio: ['pipe', 'pipe', 'pipe'] });
      proc.stdin?.write(testSecret);
      proc.stdin?.end();
      await proc;
      
      await run('kubectl', ['delete', 'secret', 'test-secret', '-n', ns], { stdio: 'pipe' });
      logSuccess('âœ“ Can create secrets in namespace');
    } catch (error) {
      validationErrors.push('Cannot create secrets in namespace - check IAM permissions');
    }
    
    // Check if we can create StatefulSets (needed for Percona)
    try {
      const testSts = `apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: test-sts
  namespace: ${ns}
spec:
  serviceName: test-service
  replicas: 0
  selector:
    matchLabels:
      app: test
  template:
    metadata:
      labels:
        app: test
    spec:
      containers:
      - name: test
        image: busybox:1.35
        command: ['sleep', '3600']`;
        
      const proc = execa('kubectl', ['apply', '-f', '-'], { stdio: ['pipe', 'pipe', 'pipe'] });
      proc.stdin?.write(testSts);
      proc.stdin?.end();
      await proc;
      
      await run('kubectl', ['delete', 'statefulset', 'test-sts', '-n', ns], { stdio: 'pipe' });
      logSuccess('âœ“ Can create StatefulSets in namespace');
    } catch (error) {
      validationErrors.push('Cannot create StatefulSets in namespace - check IAM permissions');
    }
  } catch (error) {
    validationWarnings.push(`Failed to check IAM permissions: ${error}`);
  }
  
  try {
    // 7. Check for existing Percona resources
    logInfo('Checking for existing Percona resources...');
    
    const existingPxc = await execa('kubectl', ['get', 'pxc', '-n', ns, '--no-headers'], { stdio: 'pipe' });
    if (existingPxc.stdout.trim()) {
      validationWarnings.push('Existing PXC resources found in namespace. Installation may conflict.');
    }
    
    const existingSts = await execa('kubectl', ['get', 'statefulset', '-n', ns, '--no-headers'], { stdio: 'pipe' });
    if (existingSts.stdout.trim()) {
      validationWarnings.push('Existing StatefulSets found in namespace. Installation may conflict.');
    }
    
    logSuccess('âœ“ No conflicting Percona resources found');
  } catch (error) {
    // This is expected if no resources exist
    logSuccess('âœ“ No existing Percona resources found');
  }
  
  // 8. Summary
  logInfo('=== Validation Summary ===');
  
  if (validationErrors.length > 0) {
    logError('âŒ Validation failed with errors:');
    validationErrors.forEach(error => logError(`  - ${error}`));
    throw new Error(`EKS cluster validation failed: ${validationErrors.join(', ')}`);
  }
  
  if (validationWarnings.length > 0) {
    logWarn('âš ï¸  Validation completed with warnings:');
    validationWarnings.forEach(warning => logWarn(`  - ${warning}`));
  }
  
  if (validationErrors.length === 0 && validationWarnings.length === 0) {
    logSuccess('âœ… EKS cluster validation passed - ready for Percona installation');
  } else if (validationErrors.length === 0) {
    logSuccess('âœ… EKS cluster validation passed with warnings - proceeding with installation');
  }
}

async function ensureNamespace(ns: string) {
  try {
    await run('kubectl', ['get', 'ns', ns], { stdio: 'pipe' });
  } catch {
    await run('kubectl', ['create', 'namespace', ns]);
  }
}

async function addRepos(repoUrl: string) {
  try {
    await run('helm', ['repo', 'add', 'percona', repoUrl], { stdio: 'pipe' });
  } catch (err) {
    // Repo already exists, that's fine
    logInfo('Percona repo already exists, continuing...');
  }
  await run('helm', ['repo', 'update']);
}

async function installOperator(ns: string) {
  logInfo('Installing Percona operator via Helm...');
  try {
    await run('helm', ['upgrade', '--install', 'percona-operator', 'percona/pxc-operator', '-n', ns]);
    logSuccess('Percona operator Helm chart installed successfully');
  } catch (error) {
    logError(`Failed to install Percona operator: ${error}`);
    throw error;
  }
}

function clusterValues(nodes: number, accountId: string): string {
  return `pxc:
  size: ${nodes}
  resources:
    requests:
      memory: 1Gi
      cpu: 500m
    limits:
      memory: 2Gi
      cpu: 1
  persistence:
    enabled: true
    size: 20Gi
    accessMode: ReadWriteOnce
    storageClass: gp3
haproxy:
  enabled: false
proxysql:
  enabled: true
  size: 3
  image: percona/proxysql2:2.7.3
  resources:
    requests:
      memory: 256Mi
      cpu: 100m
    limits:
      memory: 512Mi
      cpu: 500m
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: app.kubernetes.io/name
            operator: In
            values:
            - proxysql
        topologyKey: topology.kubernetes.io/zone
  volumeSpec:
    persistentVolumeClaim:
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi
      storageClassName: gp3
backup:
  enabled: true
  storages:
    s3-backup:
      type: s3
      s3:
        bucket: percona-backups-${accountId}
        region: us-east-1
        credentialsSecret: percona-backup-s3-credentials
  schedule:
    - name: "daily-backup"
      schedule: "0 2 * * *"
      retention:
        type: "count"
        count: 7
        deleteFromStorage: true
      storageName: s3-backup
    - name: "weekly-backup"
      schedule: "0 1 * * 0"
      retention:
        type: "count"
        count: 4
        deleteFromStorage: true
      storageName: s3-backup
`;}

async function createStorageClass() {
  logInfo('Creating gp3 storage class...');
  const storageClassYaml = `apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gp3
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
provisioner: ebs.csi.aws.com
allowVolumeExpansion: true
parameters:
  type: gp3
  fsType: xfs
  encrypted: "true"
reclaimPolicy: Delete
volumeBindingMode: WaitForFirstConsumer`;

  const { execa } = await import('execa');
  const proc = execa('kubectl', ['apply', '-f', '-'], { stdio: ['pipe', 'inherit', 'inherit'] });
  proc.stdin?.write(storageClassYaml);
  proc.stdin?.end();
  await proc;

  // Remove default from gp2
  try {
    await run('kubectl', ['patch', 'storageclass', 'gp2', '-p', '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}']);
  } catch (error) {
    logInfo('gp2 storage class not found or already not default');
  }

  logSuccess('Storage class created');
}

async function createS3BackupBucket(region: string) {
  logInfo('Creating S3 backup bucket...');
  const { execa } = await import('execa');
  
  try {
    // Get AWS account ID
    const accountResult = await execa('aws', ['sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], { stdio: 'pipe' });
    const accountId = accountResult.stdout.trim();
    
    const bucketName = `percona-backups-${accountId}`;
    
    // Create S3 bucket
    await run('aws', ['s3', 'mb', `s3://${bucketName}`, '--region', region]);
    
    // Enable versioning
    await run('aws', ['s3api', 'put-bucket-versioning', '--bucket', bucketName, '--versioning-configuration', 'Status=Enabled', '--region', region]);
    
    // Enable server-side encryption
    await run('aws', ['s3api', 'put-bucket-encryption', '--bucket', bucketName, '--server-side-encryption-configuration', JSON.stringify({
      Rules: [{
        ApplyServerSideEncryptionByDefault: {
          SSEAlgorithm: 'AES256'
        }
      }]
    }), '--region', region]);
    
    // Enable MFA delete protection (optional but recommended for production)
    try {
      await run('aws', ['s3api', 'put-bucket-versioning', '--bucket', bucketName, '--versioning-configuration', 'Status=Enabled,MFADelete=Disabled', '--region', region]);
    } catch (error) {
      logInfo('MFA delete configuration skipped (requires MFA device)');
    }
    
    // Set basic lifecycle policy for cost optimization
    const lifecyclePolicy = {
      Rules: [{
        ID: 'BackupLifecycle',
        Status: 'Enabled',
        Filter: {
          Prefix: ''
        },
        Transitions: [
          {
            Days: 30,
            StorageClass: 'STANDARD_IA'
          }
        ],
        Expiration: {
          Days: 2555  // 7 years retention
        }
      }]
    };
    
    // Write lifecycle policy to temporary file
    const { writeFileSync, unlinkSync } = await import('fs');
    const lifecycleFile = '/tmp/lifecycle-policy.json';
    writeFileSync(lifecycleFile, JSON.stringify(lifecyclePolicy, null, 2));
    
    try {
      await run('aws', ['s3api', 'put-bucket-lifecycle-configuration', '--bucket', bucketName, '--lifecycle-configuration', `file://${lifecycleFile}`, '--region', region]);
    } catch (lifecycleError) {
      logWarn(`Failed to set lifecycle policy: ${lifecycleError}`);
      logInfo('Continuing without lifecycle policy...');
    } finally {
      // Clean up temporary file
      try {
        unlinkSync(lifecycleFile);
      } catch (cleanupError) {
        // Ignore cleanup errors
      }
    }
    
    // Enable public access blocking for security
    await run('aws', ['s3api', 'put-public-access-block', '--bucket', bucketName, '--public-access-block-configuration', JSON.stringify({
      BlockPublicAcls: true,
      IgnorePublicAcls: true,
      BlockPublicPolicy: true,
      RestrictPublicBuckets: true
    }), '--region', region]);
    
    logSuccess(`S3 backup bucket created: ${bucketName}`);
    return bucketName;
  } catch (error) {
    logWarn(`Failed to create S3 bucket: ${error}`);
    throw error;
  }
}

async function createS3CredentialsSecret(ns: string, region: string) {
  logInfo('Creating S3 credentials secret...');
  const { execa } = await import('execa');
  
  try {
    // Get AWS account ID
    const accountResult = await execa('aws', ['sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], { stdio: 'pipe' });
    const accountId = accountResult.stdout.trim();
    
    const userName = 'percona-backup-user';
    const policyName = 'PerconaBackupPolicy';
    
    // Create IAM user for Percona backups
    try {
      const userResult = await execa('aws', ['iam', 'create-user', '--user-name', userName], { stdio: 'pipe' });
      logInfo(`Created IAM user: ${userName}`);
    } catch (error) {
      if (error.toString().includes('EntityAlreadyExists') || error.toString().includes('User with name percona-backup-user already exists')) {
        logInfo(`IAM user ${userName} already exists`);
      } else {
        logWarn(`Unexpected error creating IAM user: ${error}`);
        throw error;
      }
    }
    
    // Create IAM policy for S3 backup access
    const policyDocument = {
      Version: '2012-10-17',
      Statement: [{
        Effect: 'Allow',
        Action: [
          's3:GetObject',
          's3:PutObject',
          's3:DeleteObject',
          's3:ListBucket'
        ],
        Resource: [
          `arn:aws:s3:::percona-backups-${accountId}`,
          `arn:aws:s3:::percona-backups-${accountId}/*`
        ]
      }]
    };
    
    try {
      await execa('aws', ['iam', 'create-policy', '--policy-name', policyName, '--policy-document', JSON.stringify(policyDocument)], { stdio: 'pipe' });
      logInfo(`Created IAM policy: ${policyName}`);
    } catch (error) {
      if (error.toString().includes('EntityAlreadyExists')) {
        logInfo(`IAM policy ${policyName} already exists`);
      } else {
        throw error;
      }
    }
    
    // Attach policy to user
    try {
      await execa('aws', ['iam', 'attach-user-policy', '--user-name', userName, '--policy-arn', `arn:aws:iam::${accountId}:policy/${policyName}`], { stdio: 'pipe' });
    } catch (error) {
      if (error.toString().includes('EntityAlreadyExists')) {
        logInfo(`Policy already attached to user ${userName}`);
      } else {
        throw error;
      }
    }
    
    // Check if user already has access keys
    let accessKey, secretKey;
    try {
      const existingKeysResult = await execa('aws', ['iam', 'list-access-keys', '--user-name', userName], { stdio: 'pipe' });
      const existingKeys = JSON.parse(existingKeysResult.stdout);
      
      if (existingKeys.AccessKeyMetadata && existingKeys.AccessKeyMetadata.length > 0) {
        logInfo(`User ${userName} already has access keys, using existing ones`);
        // Get the existing access key ID
        const existingKeyId = existingKeys.AccessKeyMetadata[0].AccessKeyId;
        
        // We can't retrieve the secret key, so we need to create a new one
        // First delete the old one
        await run('aws', ['iam', 'delete-access-key', '--user-name', userName, '--access-key-id', existingKeyId]);
        logInfo('Deleted existing access key, creating new one');
      }
    } catch (error) {
      logInfo('No existing access keys found or error checking, creating new ones');
    }
    
    // Create access key for the user
    const keyResult = await execa('aws', ['iam', 'create-access-key', '--user-name', userName], { stdio: 'pipe' });
    const keyData = JSON.parse(keyResult.stdout);
    accessKey = keyData.AccessKey.AccessKeyId;
    secretKey = keyData.AccessKey.SecretAccessKey;
    
    // Create Kubernetes secret
    const secretYaml = `apiVersion: v1
kind: Secret
metadata:
  name: percona-backup-s3-credentials
  namespace: ${ns}
type: Opaque
data:
  AWS_ACCESS_KEY_ID: ${Buffer.from(accessKey).toString('base64')}
  AWS_SECRET_ACCESS_KEY: ${Buffer.from(secretKey).toString('base64')}`;

    const proc = execa('kubectl', ['apply', '-f', '-'], { stdio: ['pipe', 'inherit', 'inherit'] });
    proc.stdin?.write(secretYaml);
    proc.stdin?.end();
    await proc;

    logSuccess('S3 credentials secret created');
  } catch (error) {
    logWarn(`Failed to create S3 credentials secret: ${error}`);
    throw error;
  }
}

async function installCluster(ns: string, name: string, nodes: number, accountId: string) {
  const values = clusterValues(nodes, accountId);
  const { execa } = await import('execa');
  const proc = execa('helm', ['upgrade', '--install', name, 'percona/pxc-db', '-n', ns, '-f', '-'], { stdio: ['pipe', 'inherit', 'inherit'] });
  proc.stdin?.write(values);
  proc.stdin?.end();
  await proc;
}

async function checkProxySQLIssues(ns: string, name: string) {
  const { execa } = await import('execa');
  
  try {
    // Check ProxySQL StatefulSet
    const stsResult = await execa('kubectl', ['get', 'statefulset', '-n', ns, '-l', 'app.kubernetes.io/name=proxysql', '-o', 'json'], { stdio: 'pipe' });
    const stsData = JSON.parse(stsResult.stdout);
    
    if (stsData.items && stsData.items.length > 0) {
      const sts = stsData.items[0];
      const readyReplicas = sts.status.readyReplicas || 0;
      const desiredReplicas = sts.spec.replicas || 0;
      
      logInfo(`ProxySQL StatefulSet: ${readyReplicas}/${desiredReplicas} ready`);
      
      if (readyReplicas < desiredReplicas) {
        logWarn(`ProxySQL StatefulSet not ready: ${readyReplicas}/${desiredReplicas}`);
        
        // Check for specific issues
        if (sts.status.conditions) {
          sts.status.conditions.forEach((condition: any) => {
            if (condition.type === 'Ready' && condition.status !== 'True') {
              logWarn(`ProxySQL StatefulSet condition: ${condition.type}=${condition.status} - ${condition.message}`);
            }
          });
        }
      }
    }
    
    // Check ProxySQL pods for specific issues
    const podsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', 'app.kubernetes.io/name=proxysql', '-o', 'json'], { stdio: 'pipe' });
    const podsData = JSON.parse(podsResult.stdout);
    
    if (podsData.items) {
      podsData.items.forEach((pod: any) => {
        const podName = pod.metadata.name;
        const phase = pod.status.phase;
        const ready = pod.status.containerStatuses?.[0]?.ready || false;
        
        logInfo(`ProxySQL pod ${podName}: ${phase}, ready: ${ready}`);
        
        if (phase === 'Pending' || phase === 'Failed' || !ready) {
          logWarn(`ProxySQL pod ${podName} has issues: ${phase}`);
          
          // Check container status
          if (pod.status.containerStatuses) {
            pod.status.containerStatuses.forEach((container: any) => {
              if (container.state.waiting) {
                logWarn(`  Container waiting: ${container.state.waiting.reason} - ${container.state.waiting.message}`);
              }
              if (container.state.terminated) {
                logWarn(`  Container terminated: ${container.state.terminated.reason} - ${container.state.terminated.message}`);
              }
            });
          }
        }
      });
    }
    
  } catch (error) {
    logWarn(`Error checking ProxySQL issues: ${error}`);
  }
}

async function waitForOperatorReady(ns: string) {
  logInfo('Waiting for Percona operator to be ready...');
  const { execa } = await import('execa');
  const startTime = Date.now();
  const timeout = 5 * 60 * 1000; // 5 minutes
  
  while (Date.now() - startTime < timeout) {
    try {
      const elapsed = Math.round((Date.now() - startTime) / 1000);
      
      // Check for any pods in the namespace first
      const allPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const allPods = allPodsResult.stdout.trim().split('\n').filter(line => line.trim());
      logInfo(`Found ${allPods.length} total pods in namespace ${ns}`);
      
      if (allPods.length > 0) {
        logInfo('All pods in namespace:');
        allPods.forEach((pod, index) => {
          logInfo(`  ${index + 1}. ${pod}`);
        });
      }
      
      // Check for operator pods specifically
      const operatorPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', 'app.kubernetes.io/name=percona-xtradb-cluster-operator', '--no-headers'], { stdio: 'pipe' });
      const operatorPods = operatorPodsResult.stdout.trim().split('\n').filter(line => line.trim());
      
      logInfo(`Found ${operatorPods.length} operator pods with label 'app.kubernetes.io/name=percona-xtradb-cluster-operator'`);
      
      if (operatorPods.length === 0) {
        // Try alternative labels
        logInfo('Trying alternative operator pod labels...');
        const altLabels = [
          'app.kubernetes.io/name=percona-xtradb-cluster-operator',
          'app=percona-xtradb-cluster-operator',
          'name=percona-xtradb-cluster-operator',
          'app.kubernetes.io/component=operator'
        ];
        
        let foundOperatorPods: string[] = [];
        let workingLabel = '';
        
        for (const label of altLabels) {
          try {
            const altResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', label, '--no-headers'], { stdio: 'pipe' });
            const altPods = altResult.stdout.trim().split('\n').filter(line => line.trim());
            if (altPods.length > 0) {
              logInfo(`Found ${altPods.length} pods with label '${label}':`);
              altPods.forEach((pod, index) => {
                logInfo(`  ${index + 1}. ${pod}`);
              });
              foundOperatorPods = altPods;
              workingLabel = label;
              break; // Use the first working label
            }
          } catch (altError) {
            // Ignore label errors
          }
        }
        
        if (foundOperatorPods.length > 0) {
          logInfo(`Using operator pods found with label '${workingLabel}'`);
          const readyPods = foundOperatorPods.filter(line => {
            const parts = line.split(/\s+/);
            const ready = parts[1];
            return ready.includes('/') && ready.split('/')[0] === ready.split('/')[1];
          });
          
          if (readyPods.length > 0) {
            logSuccess('Percona operator is ready');
            return;
          } else {
            logInfo(`Percona operator pods starting: ${foundOperatorPods.length} found, ${readyPods.length} ready (${elapsed}s elapsed)`);
          }
        } else {
          logInfo(`Percona operator pods not found yet... (${elapsed}s elapsed)`);
        }
      } else {
        logInfo(`Operator pods found: ${operatorPods.length}`);
        operatorPods.forEach((pod, index) => {
          logInfo(`  ${index + 1}. ${pod}`);
        });
        
        const readyPods = operatorPods.filter(line => {
          const parts = line.split(/\s+/);
          const ready = parts[1];
          return ready.includes('/') && ready.split('/')[0] === ready.split('/')[1];
        });
        
        if (readyPods.length > 0) {
          logSuccess('Percona operator is ready');
          return;
        } else {
          logInfo(`Percona operator pods starting: ${operatorPods.length} found, ${readyPods.length} ready (${elapsed}s elapsed)`);
        }
      }
      
      await new Promise(resolve => setTimeout(resolve, 10000)); // Wait 10 seconds
    } catch (error) {
      const elapsed = Math.round((Date.now() - startTime) / 1000);
      logWarn(`Error checking operator status (${elapsed}s): ${error}`);
      await new Promise(resolve => setTimeout(resolve, 10000));
    }
  }
  
  throw new Error('Percona operator did not become ready within 5 minutes');
}

async function waitForClusterReady(ns: string, name: string, nodes: number) {
  const pxcResourceName = `${name}-pxc-db`;
  logInfo(`Waiting for Percona cluster ${pxcResourceName} to be ready...`);
  const { execa } = await import('execa');
  const startTime = Date.now();
  const timeout = 15 * 60 * 1000; // 15 minutes (reduced from 30)
  
  while (Date.now() - startTime < timeout) {
    try {
      // First check if PXC custom resource exists
      let pxcExists = false;
      try {
        await execa('kubectl', ['get', 'pxc', pxcResourceName, '-n', ns], { stdio: 'pipe' });
        pxcExists = true;
      } catch (error) {
        if (error.toString().includes('NotFound')) {
          logInfo(`PXC resource ${pxcResourceName} not found yet, waiting for operator to create it...`);
        } else {
          logWarn(`Error checking PXC resource existence: ${error}`);
        }
      }
      
      if (!pxcExists) {
        // Check if the operator is running
        try {
          const operatorPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', 'app.kubernetes.io/name=percona-xtradb-cluster-operator', '--no-headers'], { stdio: 'pipe' });
          const operatorPods = operatorPodsResult.stdout.trim().split('\n').filter(line => line.trim());
          
          if (operatorPods.length === 0) {
            logWarn('Percona operator not found, waiting for installation...');
          } else {
            const readyOperators = operatorPods.filter(line => line.includes('Running'));
            if (readyOperators.length === 0) {
              logWarn('Percona operator pods not ready yet...');
            } else {
              logInfo('Percona operator is running, waiting for PXC resource creation...');
            }
          }
        } catch (error) {
          logWarn(`Error checking operator status: ${error}`);
        }
        
        // Wait and continue
        await new Promise(resolve => setTimeout(resolve, 30000));
        continue;
      }
      
      // Check PXC custom resource status
      const pxcResult = await execa('kubectl', ['get', 'pxc', pxcResourceName, '-n', ns, '-o', 'json'], { stdio: 'pipe' });
      const pxc = JSON.parse(pxcResult.stdout);
      
      // Debug: log the actual status structure
      if (pxc.status) {
        logInfo(`Debug - PXC status: state=${pxc.status.state}, pxc.status=${pxc.status.pxc?.status}, proxysql.status=${pxc.status.proxysql?.status}`);
      }
      
      const pxcCount = typeof pxc.status?.pxc === 'number' ? pxc.status.pxc : (pxc.status?.pxc?.ready || 0);
      const proxysqlCount = typeof pxc.status?.proxysql === 'number' ? pxc.status.proxysql : (pxc.status?.proxysql?.ready || 0);
      const status = pxc.status?.status || 'unknown';
      const elapsed = Math.round((Date.now() - startTime) / 1000);
      
      // Explain what the numbers mean
      if (elapsed === 0 || elapsed % 60 === 0) { // Show explanation every minute
        logInfo(`ðŸ’¡ Status Guide: PXC = Percona XtraDB Cluster nodes, ProxySQL = Database proxy pods, Status 'unknown' = Still initializing`);
      }
      logInfo(`ðŸ“Š Cluster Progress: PXC nodes ${pxcCount}/${nodes}, ProxySQL pods ${proxysqlCount}/3, Status: ${status} (${elapsed}s elapsed)`);
      
      // Check PXC pods
      const podsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', 'app.kubernetes.io/name=percona-xtradb-cluster', '--no-headers'], { stdio: 'pipe' });
      const podLines = podsResult.stdout.trim().split('\n').filter(line => line.includes(`${pxcResourceName}-pxc-`));
      logInfo(`ðŸ” PXC pods found: ${podLines.length}/${nodes}`);
      
      // Check ProxySQL pods with multiple label selectors
      try {
        let proxysqlPodsFound = 0;
        const proxysqlLabels = [
          'app.kubernetes.io/component=proxysql',
          'app.kubernetes.io/name=proxysql',
          'app=proxysql'
        ];
        
        for (const label of proxysqlLabels) {
          try {
            const proxysqlPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', label, '--no-headers'], { stdio: 'pipe' });
            const proxysqlPodLines = proxysqlPodsResult.stdout.trim().split('\n').filter(line => line.trim());
            if (proxysqlPodLines.length > 0) {
              logInfo(`ðŸ” ProxySQL pods found with label '${label}': ${proxysqlPodLines.length}`);
              proxysqlPodLines.forEach((line, index) => {
                const parts = line.split(/\s+/);
                const name = parts[0];
                const ready = parts[1];
                const status = parts[2];
                logInfo(`  ProxySQL pod ${index + 1}: ${name} (${ready}, ${status})`);
              });
              proxysqlPodsFound = Math.max(proxysqlPodsFound, proxysqlPodLines.length);
            }
          } catch (labelError) {
            // Try next label
          }
        }
        
        if (proxysqlPodsFound === 0) {
          logInfo(`ðŸ” No ProxySQL pods found with any label selector`);
        } else if (proxysqlPodsFound === 3) {
          // Check if ProxySQL pods are distributed across AZs
          try {
            const proxysqlPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', 'app.kubernetes.io/component=proxysql', '-o', 'json'], { stdio: 'pipe' });
            const proxysqlPods = JSON.parse(proxysqlPodsResult.stdout);
            const zones = new Set();
            
            for (const pod of proxysqlPods.items) {
              if (pod.spec.nodeName) {
                try {
                  const nodeResult = await execa('kubectl', ['get', 'node', pod.spec.nodeName, '-o', 'json'], { stdio: 'pipe' });
                  const node = JSON.parse(nodeResult.stdout);
                  const zone = node.metadata.labels?.['topology.kubernetes.io/zone'] || 
                              node.metadata.labels?.['failure-domain.beta.kubernetes.io/zone'] || 
                              'Unknown';
                  zones.add(zone);
                } catch (nodeError) {
                  zones.add('Unknown');
                }
              } else {
                zones.add('Pending');
              }
            }
            
            if (zones.size >= 2) {
              logSuccess(`âœ“ ProxySQL pods distributed across ${zones.size} availability zones: ${Array.from(zones).join(', ')}`);
            } else if (zones.size === 1) {
              logWarn(`âš ï¸  All ProxySQL pods in same zone (${Array.from(zones)[0]}). Consider multi-AZ deployment for high availability.`);
            }
          } catch (error) {
            // Ignore zone checking errors
          }
        }
      } catch (error) {
        logWarn(`Error checking ProxySQL pods: ${error}`);
      }
      
      // Check for any failed pods
      try {
        const failedPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '--field-selector=status.phase=Failed', '--no-headers'], { stdio: 'pipe' });
        const failedPods = failedPodsResult.stdout.trim().split('\n').filter(line => line.trim());
        if (failedPods.length > 0) {
          logWarn(`Found ${failedPods.length} failed pods:`);
          failedPods.forEach(pod => logWarn(`  Failed: ${pod}`));
        }
      } catch (error) {
        // Ignore errors here as this is just for debugging
      }
      
      // Check Kubernetes events for errors (every 2 minutes)
      if (elapsed % 120 === 0 && elapsed > 0) {
        try {
          const eventsResult = await execa('kubectl', ['get', 'events', '-n', ns, '--sort-by=.lastTimestamp', '--field-selector=type=Warning', '--no-headers'], { stdio: 'pipe' });
          const warningEvents = eventsResult.stdout.trim().split('\n').filter(line => line.trim()).slice(-5); // Last 5 warnings
          if (warningEvents.length > 0) {
            logWarn(`Recent warning events:`);
            warningEvents.forEach(event => logWarn(`  ${event}`));
          }
        } catch (error) {
          // Ignore errors here as this is just for debugging
        }
      }
      
      // Check ProxySQL issues (every 2 minutes)
      if (elapsed % 120 === 0 && elapsed > 0) {
        logInfo('=== Checking ProxySQL status ===');
        await checkProxySQLIssues(ns, name);
      }
      
      // Check ProxySQL pod logs if there are issues (every 3 minutes)
      if (elapsed % 180 === 0 && elapsed > 0) {
        try {
          const proxysqlPodsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '-l', 'app.kubernetes.io/name=proxysql', '--no-headers', '-o', 'custom-columns=NAME:.metadata.name'], { stdio: 'pipe' });
          const proxysqlPodNames = proxysqlPodsResult.stdout.trim().split('\n').filter(name => name.trim());
          
          for (const podName of proxysqlPodNames.slice(0, 2)) { // Check first 2 ProxySQL pods
            try {
              const logsResult = await execa('kubectl', ['logs', podName, '-n', ns, '--tail=10', '--since=2m'], { stdio: 'pipe' });
              const logs = logsResult.stdout.trim();
              if (logs) {
                logInfo(`ProxySQL pod ${podName} recent logs:`);
                logs.split('\n').forEach(line => {
                  if (line.trim()) {
                    logInfo(`  ${line}`);
                  }
                });
              }
            } catch (logError) {
              logWarn(`Could not get logs from ProxySQL pod ${podName}: ${logError}`);
            }
          }
        } catch (error) {
          logWarn(`Error checking ProxySQL pod logs: ${error}`);
        }
      }
      
              if (podLines.length >= nodes) {
                const allReady = podLines.every(line => {
                  const parts = line.split(/\s+/);
                  const ready = parts[1];
                  return ready.includes('/') && ready.split('/')[0] === ready.split('/')[1];
                });
                
                // Check if cluster is ready based on PXC status
                const clusterReady = pxc.status?.state === 'ready' || 
                                   (pxcCount >= nodes && pxc.status?.proxysql?.status === 'ready');
                
                if (allReady && clusterReady) {
                  logSuccess(`ðŸŽ‰ Percona cluster ${pxcResourceName} is ready with ${nodes} PXC nodes and 3 ProxySQL pods!`);
                  logSuccess(`ðŸ“Š Final Status: PXC ${pxcCount}/${nodes}, ProxySQL ${proxysqlCount}/3, State: ${pxc.status?.state || status}`);
                  return;
                }
              }
      
      await new Promise(resolve => setTimeout(resolve, 30000)); // Wait 30 seconds
    } catch (error) {
      const elapsed = Math.round((Date.now() - startTime) / 1000);
      logWarn(`Error checking cluster status (${elapsed}s): ${error}`);
      await new Promise(resolve => setTimeout(resolve, 30000));
    }
  }
  
  throw new Error(`Percona cluster ${pxcResourceName} did not become ready within 15 minutes. Check the logs above for ProxySQL and PXC pod issues.`);
}

async function expandVolumes(ns: string, name: string, newSize: string) {
  logInfo(`Expanding Percona volumes to ${newSize}...`);
  
  try {
    // Get all PVCs for the cluster
    const { execa } = await import('execa');
    const pvcResult = await execa('kubectl', ['get', 'pvc', '-n', ns, '-l', `app.kubernetes.io/instance=${name}`, '--no-headers', '-o', 'custom-columns=NAME:.metadata.name'], { stdio: 'pipe' });
    const pvcs = pvcResult.stdout.trim().split('\n').filter(name => name.trim());
    
    if (pvcs.length === 0) {
      logWarn('No PVCs found for the cluster');
      return;
    }
    
    // Expand each PVC
    for (const pvcName of pvcs) {
      logInfo(`Expanding PVC: ${pvcName}`);
      await run('kubectl', ['patch', 'pvc', pvcName, '-n', ns, '-p', `{"spec":{"resources":{"requests":{"storage":"${newSize}"}}}}`]);
    }
    
    // Wait for expansion to complete
    logInfo('Waiting for volume expansion to complete...');
    await run('kubectl', ['wait', '--for=condition=FileSystemResizePending', 'pvc', '--all', '-n', ns, '--timeout=300s']);
    
    // Restart the cluster to pick up the new size
    logInfo('Restarting Percona cluster to apply volume expansion...');
    await run('kubectl', ['rollout', 'restart', 'statefulset', `${name}-pxc-db-pxc`, '-n', ns]);
    
    logSuccess(`Volumes expanded to ${newSize} successfully`);
  } catch (error) {
    logError(`Failed to expand volumes: ${error}`);
    throw error;
  }
}

async function uninstall(ns: string, name: string) {
  logInfo('Uninstalling Percona cluster and operator...');
  
  const { execa } = await import('execa');
  let pxcDeleted = false;
  const maxAttempts = 3;
  
  // First check if PXC resource exists at all
  let pxcExists = false;
  try {
    const initialCheck = await execa('kubectl', ['get', 'pxc', name, '-n', ns], { stdio: 'pipe' });
    pxcExists = (initialCheck.exitCode === 0);
  } catch (error) {
    pxcExists = false;
  }
  
  if (!pxcExists) {
    logSuccess('âœ“ PXC custom resource not found - already deleted');
    pxcDeleted = true;
  } else {
    logInfo(`PXC resource exists, attempting deletion with ${maxAttempts} attempts...`);
    
    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      logInfo(`Attempt ${attempt}/${maxAttempts}: Deleting PXC resource...`);
      
      try {
        if (attempt === 1) {
          // Normal deletion
          await run('kubectl', ['delete', 'pxc', name, '-n', ns, '--timeout=30s'], { stdio: 'pipe' });
          logInfo('Normal deletion command completed');
        } else if (attempt === 2) {
          // Force deletion with finalizer removal
          await run('kubectl', ['patch', 'pxc', name, '-n', ns, '-p', '{"metadata":{"finalizers":[]}}', '--type=merge'], { stdio: 'pipe' });
          await run('kubectl', ['delete', 'pxc', name, '-n', ns, '--force', '--grace-period=0'], { stdio: 'pipe' });
          logInfo('Force deletion command completed');
        } else {
          // Final attempt: aggressive cleanup
          logError(`Final attempt ${attempt}: Aggressive PXC cleanup...`);
          try {
            // Remove all finalizers multiple times
            await run('kubectl', ['patch', 'pxc', name, '-n', ns, '-p', '{"metadata":{"finalizers":[]}}', '--type=merge'], { stdio: 'pipe' });
            await run('kubectl', ['patch', 'pxc', name, '-n', ns, '-p', '{"metadata":{"finalizers":null}}', '--type=merge'], { stdio: 'pipe' });
            
            // Try multiple deletion approaches
            try {
              await run('kubectl', ['delete', 'pxc', name, '-n', ns, '--force', '--grace-period=0'], { stdio: 'pipe' });
            } catch (deleteError) {
              // Try with different flags
              await run('kubectl', ['delete', 'pxc', name, '-n', ns, '--cascade=orphan'], { stdio: 'pipe' });
            }
            
            logError('Aggressive cleanup completed');
          } catch (aggressiveError) {
            logError(`Aggressive cleanup failed: ${aggressiveError}`);
          }
        }
      } catch (error) {
        logWarn(`Deletion attempt ${attempt} failed: ${error}`);
      }
      
      // Wait for Kubernetes to process the deletion
      await new Promise(resolve => setTimeout(resolve, 3000));
      
      // Verify if deletion was successful
      try {
        const verifyResult = await execa('kubectl', ['get', 'pxc', name, '-n', ns], { stdio: 'pipe' });
        if (verifyResult.exitCode !== 0) {
          logSuccess(`âœ“ PXC resource successfully deleted on attempt ${attempt}`);
          pxcDeleted = true;
          break;
        } else {
          logWarn(`PXC resource still exists after attempt ${attempt}`);
        }
      } catch (error) {
        logSuccess(`âœ“ PXC resource successfully deleted on attempt ${attempt}`);
        pxcDeleted = true;
        break;
      }
    }
  }
  
  // CRITICAL: If PXC still exists, abort the entire uninstall
  if (!pxcDeleted) {
    logError('âŒ FATAL: PXC resource could not be deleted after all attempts!');
    logError('This will prevent namespace deletion. The uninstall cannot continue.');
    logError('Manual intervention required: kubectl patch pxc <name> -n <ns> -p \'{"metadata":{"finalizers":[]}}\' --type=merge');
    throw new Error('CRITICAL: PXC resource deletion failed - uninstall aborted');
  }
  
  // Delete resources in correct order (dependent resources first)
  
  // 1. Delete Pods first (they depend on StatefulSets)
  try {
    logInfo('Deleting Pods...');
    await run('kubectl', ['delete', 'pods', '--all', '-n', ns, '--timeout=30s'], { stdio: 'pipe' });
    logSuccess('Pods deleted successfully');
  } catch (error) {
    if (error.toString().includes('NotFound') || error.toString().includes('no resources found')) {
      logInfo('No Pods found or already deleted');
    } else {
      logWarn(`Error deleting Pods: ${error}`);
    }
  }
  
  // 2. Delete StatefulSets (they depend on PVCs)
  try {
    logInfo('Deleting StatefulSets...');
    await run('kubectl', ['delete', 'statefulset', '--all', '-n', ns, '--timeout=30s'], { stdio: 'pipe' });
    logSuccess('StatefulSets deleted successfully');
  } catch (error) {
    if (error.toString().includes('NotFound') || error.toString().includes('no resources found')) {
      logInfo('No StatefulSets found or already deleted');
    } else {
      logWarn(`Error deleting StatefulSets: ${error}`);
    }
  }
  
  // 3. Delete Services
  try {
    logInfo('Deleting Services...');
    await run('kubectl', ['delete', 'service', '--all', '-n', ns], { stdio: 'pipe' });
    logSuccess('Services deleted successfully');
  } catch (error) {
    if (error.toString().includes('NotFound') || error.toString().includes('no resources found')) {
      logInfo('No Services found or already deleted');
    } else {
      logWarn(`Error deleting Services: ${error}`);
    }
  }
  
  // 4. Delete PVCs (they can have finalizers)
  try {
    logInfo('Deleting PVCs...');
    await run('kubectl', ['delete', 'pvc', '--all', '-n', ns, '--timeout=60s'], { stdio: 'pipe' });
    logSuccess('PVCs deleted successfully');
  } catch (error) {
    if (error.toString().includes('NotFound') || error.toString().includes('no resources found')) {
      logInfo('No PVCs found or already deleted');
    } else if (error.toString().includes('timeout')) {
      logWarn('PVC deletion timed out, forcing cleanup...');
      try {
        // Force delete PVCs that are stuck
        const pvcResult = await execa('kubectl', ['get', 'pvc', '-n', ns, '--no-headers', '-o', 'custom-columns=NAME:.metadata.name'], { stdio: 'pipe' });
        const pvcNames = pvcResult.stdout.trim().split('\n').filter(name => name.trim());
        
        for (const pvcName of pvcNames) {
          try {
            logInfo(`Force deleting PVC: ${pvcName}`);
            // Remove finalizers first
            await run('kubectl', ['patch', 'pvc', pvcName, '-n', ns, '-p', '{"metadata":{"finalizers":[]}}', '--type=merge'], { stdio: 'pipe' });
            // Force delete with immediate termination
            await run('kubectl', ['delete', 'pvc', pvcName, '-n', ns, '--force', '--grace-period=0'], { stdio: 'pipe' });
          } catch (pvcError) {
            logWarn(`Failed to force delete PVC ${pvcName}: ${pvcError}`);
          }
        }
        logSuccess('PVCs force deleted successfully');
      } catch (forceError) {
        logWarn(`Error force deleting PVCs: ${forceError}`);
      }
    } else {
      logWarn(`Error deleting PVCs: ${error}`);
    }
  }
  
  // Delete Percona-related Secrets
  try {
    logInfo('Deleting Percona-related Secrets...');
    
    // Get all secrets and filter for Percona-related ones
    const { execa } = await import('execa');
    const secretsResult = await execa('kubectl', ['get', 'secrets', '-n', ns, '--no-headers', '-o', 'custom-columns=NAME:.metadata.name'], { stdio: 'pipe' });
    const allSecrets = secretsResult.stdout.trim().split('\n').filter(line => line.trim());
    const perconaSecrets = allSecrets.filter(secret => 
      secret.includes('percona') || 
      secret.includes('pxc') || 
      secret.includes('proxysql') ||
      secret.includes('backup') ||
      secret.includes('ssl') ||
      secret.includes('internal')
    );
    
    if (perconaSecrets.length > 0) {
      logInfo(`Found ${perconaSecrets.length} Percona-related Secrets to delete: ${perconaSecrets.join(', ')}`);
      for (const secretName of perconaSecrets) {
        try {
          await run('kubectl', ['delete', 'secret', secretName, '-n', ns]);
          logInfo(`Deleted secret: ${secretName}`);
        } catch (secretError) {
          logWarn(`Failed to delete secret ${secretName}: ${secretError}`);
        }
      }
      logSuccess('Percona-related Secrets deleted successfully');
    } else {
      logInfo('No Percona-related Secrets found to delete');
    }
  } catch (error) {
    logWarn(`Error deleting Percona-related Secrets: ${error}`);
  }
  
  // Delete Percona-related ConfigMaps
  try {
    logInfo('Deleting Percona-related ConfigMaps...');
    
    const { execa } = await import('execa');
    const cmResult = await execa('kubectl', ['get', 'configmap', '-n', ns, '--no-headers', '-o', 'custom-columns=NAME:.metadata.name'], { stdio: 'pipe' });
    const allConfigMaps = cmResult.stdout.trim().split('\n').filter(line => line.trim());
    const perconaConfigMaps = allConfigMaps.filter(cm => 
      cm.includes('percona') || 
      cm.includes('pxc') || 
      cm.includes('proxysql')
    );
    
    if (perconaConfigMaps.length > 0) {
      logInfo(`Found ${perconaConfigMaps.length} Percona-related ConfigMaps to delete: ${perconaConfigMaps.join(', ')}`);
      for (const cmName of perconaConfigMaps) {
        try {
          await run('kubectl', ['delete', 'configmap', cmName, '-n', ns]);
          logInfo(`Deleted configmap: ${cmName}`);
        } catch (cmError) {
          logWarn(`Failed to delete configmap ${cmName}: ${cmError}`);
        }
      }
      logSuccess('Percona-related ConfigMaps deleted successfully');
    } else {
      logInfo('No Percona-related ConfigMaps found to delete');
    }
  } catch (error) {
    logWarn(`Error deleting Percona-related ConfigMaps: ${error}`);
  }
  
  // Uninstall Helm releases
  logInfo('Uninstalling Helm releases...');
  
  // Check if Helm releases exist before trying to uninstall
  try {
    const { execa } = await import('execa');
    const listResult = await execa('helm', ['list', '-n', ns, '--output', 'json'], { stdio: 'pipe' });
    const releases = JSON.parse(listResult.stdout);
    
    const clusterRelease = releases.find((r: any) => r.name === name);
    const operatorRelease = releases.find((r: any) => r.name === 'percona-operator');
    
    if (clusterRelease) {
      await run('helm', ['uninstall', name, '-n', ns]);
      logSuccess(`Helm release ${name} uninstalled successfully`);
    } else {
      logInfo(`Helm release ${name} not found or already deleted`);
    }
    
    if (operatorRelease) {
      await run('helm', ['uninstall', 'percona-operator', '-n', ns]);
      logSuccess('Percona operator Helm release uninstalled successfully');
    } else {
      logInfo('Percona operator Helm release not found or already deleted');
    }
  } catch (error) {
    logWarn(`Error checking Helm releases: ${error}`);
    // Fallback to trying to uninstall anyway
    try {
      await run('helm', ['uninstall', name, '-n', ns]);
      logSuccess(`Helm release ${name} uninstalled successfully`);
    } catch (uninstallError) {
      logInfo(`Helm release ${name} not found or already deleted`);
    }
    
    try {
      await run('helm', ['uninstall', 'percona-operator', '-n', ns]);
      logSuccess('Percona operator Helm release uninstalled successfully');
    } catch (uninstallError) {
      logInfo('Percona operator Helm release not found or already deleted');
    }
  }
  
  // Delete the namespace itself
  logInfo('Deleting Percona namespace...');
  let namespaceDeleted = false;
  
  try {
    await run('kubectl', ['delete', 'namespace', ns, '--timeout=60s'], { stdio: 'pipe' });
    logInfo('Namespace deletion command completed, verifying...');
  } catch (error) {
    if (error.toString().includes('NotFound')) {
      logInfo(`Namespace ${ns} not found or already deleted`);
      namespaceDeleted = true;
    } else if (error.toString().includes('timeout')) {
      logWarn(`Namespace deletion timed out, forcing cleanup...`);
    } else {
      logWarn(`Namespace deletion failed: ${error}`);
    }
  }
  
  // If namespace deletion failed or timed out, try force deletion
  if (!namespaceDeleted) {
    try {
      // Check if namespace still exists
      const nsCheck = await execa('kubectl', ['get', 'namespace', ns], { stdio: 'pipe' });
      if (nsCheck.exitCode === 0) {
        logWarn('Namespace still exists, removing finalizers and force deleting...');
        await run('kubectl', ['patch', 'namespace', ns, '-p', '{"metadata":{"finalizers":[]}}', '--type=merge'], { stdio: 'pipe' });
        await run('kubectl', ['delete', 'namespace', ns, '--force', '--grace-period=0'], { stdio: 'pipe' });
        logInfo('Force deletion command completed, verifying...');
      } else {
        logInfo('Namespace not found - deletion successful');
        namespaceDeleted = true;
      }
    } catch (forceError) {
      logWarn(`Error during force deletion: ${forceError}`);
    }
  }
  
  // Final verification: check if namespace is actually deleted
  logInfo('=== Final verification ===');
  try {
    const finalCheck = await execa('kubectl', ['get', 'namespace', ns], { stdio: 'pipe' });
    if (finalCheck.exitCode === 0) {
      logError(`âŒ CRITICAL: Namespace ${ns} still exists after all deletion attempts!`);
      logError('This indicates the uninstall was not completely successful.');
      throw new Error(`CRITICAL: Namespace ${ns} could not be deleted - uninstall incomplete`);
    } else {
      logSuccess(`âœ“ Namespace ${ns} successfully deleted - uninstall complete`);
    }
  } catch (error) {
    if (error.message.includes('Namespace') && error.message.includes('could not be deleted')) {
      throw error; // Re-throw critical errors
    }
    logSuccess(`âœ“ Namespace ${ns} successfully deleted - uninstall complete`);
  }
  
  logSuccess('Percona cluster and operator uninstalled successfully');
}

async function verifyCleanup(ns: string, name: string) {
  const { execa } = await import('execa');
  let cleanupIssues: string[] = [];
  
  try {
    // 1. Check for remaining PXC custom resources
    logInfo('Checking for remaining PXC custom resources...');
    try {
      const pxcResult = await execa('kubectl', ['get', 'pxc', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const pxcResources = pxcResult.stdout.trim().split('\n').filter(line => line.trim());
      if (pxcResources.length > 0) {
        cleanupIssues.push(`Found ${pxcResources.length} remaining PXC resources: ${pxcResources.join(', ')}`);
      } else {
        logSuccess('âœ“ No PXC custom resources found');
      }
    } catch (error) {
      logSuccess('âœ“ No PXC custom resources found');
    }
    
    // 2. Check for remaining StatefulSets
    logInfo('Checking for remaining StatefulSets...');
    try {
      const stsResult = await execa('kubectl', ['get', 'statefulset', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const stsResources = stsResult.stdout.trim().split('\n').filter(line => line.trim());
      if (stsResources.length > 0) {
        cleanupIssues.push(`Found ${stsResources.length} remaining StatefulSets: ${stsResources.join(', ')}`);
      } else {
        logSuccess('âœ“ No StatefulSets found');
      }
    } catch (error) {
      logSuccess('âœ“ No StatefulSets found');
    }
    
    // 3. Check for remaining Services
    logInfo('Checking for remaining Services...');
    try {
      const svcResult = await execa('kubectl', ['get', 'service', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const svcResources = svcResult.stdout.trim().split('\n').filter(line => line.trim());
      if (svcResources.length > 0) {
        cleanupIssues.push(`Found ${svcResources.length} remaining Services: ${svcResources.join(', ')}`);
      } else {
        logSuccess('âœ“ No Services found');
      }
    } catch (error) {
      logSuccess('âœ“ No Services found');
    }
    
    // 4. Check for remaining PVCs
    logInfo('Checking for remaining PVCs...');
    try {
      const pvcResult = await execa('kubectl', ['get', 'pvc', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const pvcResources = pvcResult.stdout.trim().split('\n').filter(line => line.trim());
      if (pvcResources.length > 0) {
        cleanupIssues.push(`Found ${pvcResources.length} remaining PVCs: ${pvcResources.join(', ')}`);
      } else {
        logSuccess('âœ“ No PVCs found');
      }
    } catch (error) {
      logSuccess('âœ“ No PVCs found');
    }
    
    // 5. Check for remaining Pods
    logInfo('Checking for remaining Pods...');
    try {
      const podsResult = await execa('kubectl', ['get', 'pods', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const podResources = podsResult.stdout.trim().split('\n').filter(line => line.trim());
      if (podResources.length > 0) {
        cleanupIssues.push(`Found ${podResources.length} remaining Pods: ${podResources.join(', ')}`);
      } else {
        logSuccess('âœ“ No Pods found');
      }
    } catch (error) {
      logSuccess('âœ“ No Pods found');
    }
    
    // 6. Check for remaining Secrets
    logInfo('Checking for Percona-related Secrets...');
    try {
      const secretsResult = await execa('kubectl', ['get', 'secrets', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const allSecrets = secretsResult.stdout.trim().split('\n').filter(line => line.trim());
      const perconaSecrets = allSecrets.filter(secret => 
        secret.includes('percona') || 
        secret.includes('pxc') || 
        secret.includes('proxysql') ||
        secret.includes('backup')
      );
      if (perconaSecrets.length > 0) {
        cleanupIssues.push(`Found ${perconaSecrets.length} Percona-related Secrets: ${perconaSecrets.join(', ')}`);
      } else {
        logSuccess('âœ“ No Percona-related Secrets found');
      }
    } catch (error) {
      logSuccess('âœ“ No Percona-related Secrets found');
    }
    
    // 7. Check for remaining ConfigMaps
    logInfo('Checking for Percona-related ConfigMaps...');
    try {
      const cmResult = await execa('kubectl', ['get', 'configmap', '-n', ns, '--no-headers'], { stdio: 'pipe' });
      const allConfigMaps = cmResult.stdout.trim().split('\n').filter(line => line.trim());
      const perconaConfigMaps = allConfigMaps.filter(cm => 
        cm.includes('percona') || 
        cm.includes('pxc') || 
        cm.includes('proxysql')
      );
      if (perconaConfigMaps.length > 0) {
        cleanupIssues.push(`Found ${perconaConfigMaps.length} Percona-related ConfigMaps: ${perconaConfigMaps.join(', ')}`);
      } else {
        logSuccess('âœ“ No Percona-related ConfigMaps found');
      }
    } catch (error) {
      logSuccess('âœ“ No Percona-related ConfigMaps found');
    }
    
    // 8. Check for remaining Helm releases
    logInfo('Checking for remaining Helm releases...');
    try {
      const helmResult = await execa('helm', ['list', '-n', ns, '--output', 'json'], { stdio: 'pipe' });
      const releases = JSON.parse(helmResult.stdout);
      const perconaReleases = releases.filter((r: any) => 
        r.name.includes('percona') || 
        r.name.includes('pxc') || 
        r.name === name
      );
      if (perconaReleases.length > 0) {
        cleanupIssues.push(`Found ${perconaReleases.length} remaining Helm releases: ${perconaReleases.map((r: any) => r.name).join(', ')}`);
      } else {
        logSuccess('âœ“ No Helm releases found');
      }
    } catch (error) {
      logSuccess('âœ“ No Helm releases found');
    }
    
    
    // 9. Check if namespace still exists
    logInfo('Checking if Percona namespace still exists...');
    try {
      await execa('kubectl', ['get', 'namespace', ns], { stdio: 'pipe' });
      cleanupIssues.push(`Namespace ${ns} still exists`);
    } catch (error) {
      logSuccess('âœ“ Percona namespace has been deleted');
    }
    
    // 10. Summary
    logInfo('=== Cleanup Verification Summary ===');
    if (cleanupIssues.length > 0) {
      logWarn('âš ï¸  Cleanup issues found:');
      cleanupIssues.forEach(issue => logWarn(`  - ${issue}`));
      logWarn('Some resources may still exist. You may need to delete them manually.');
    } else {
      logSuccess('âœ… All Percona resources have been successfully removed');
    }
    
  } catch (error) {
    logWarn(`Error during cleanup verification: ${error}`);
  }
}

async function main() {
  const argv = await yargs(hideBin(process.argv))
    .command('install', 'Install Percona operator and 3-node cluster')
    .command('uninstall', 'Uninstall Percona cluster and operator')
    .command('expand', 'Expand Percona cluster volumes')
    .option('namespace', { type: 'string' })
    .option('name', { type: 'string' })
    .option('helmRepo', { type: 'string' })
    .option('chart', { type: 'string' })
    .option('clusterChart', { type: 'string' })
    .option('nodes', { type: 'number' })
    .option('size', { type: 'string', description: 'New volume size (e.g., 50Gi, 100Gi)' })
    .demandCommand(1)
    .strict()
    .parse();

  const action = String(argv._[0]);
  const parsed = Args.parse({
    action,
    namespace: argv.namespace,
    name: argv.name,
    helmRepo: argv.helmRepo,
    chart: argv.chart,
    clusterChart: argv.clusterChart,
    nodes: argv.nodes,
    size: argv.size,
  });

  try {
    await ensurePrereqs();
    if (parsed.action === 'install') {
      // Validate EKS cluster state before installation
      await validateEksCluster(parsed.namespace);
      
      await ensureNamespace(parsed.namespace);
      await addRepos(parsed.helmRepo);
      
      // Create storage class first
      await createStorageClass();
      
      // Get AWS account ID
      const { execa } = await import('execa');
      const accountResult = await execa('aws', ['sts', 'get-caller-identity', '--query', 'Account', '--output', 'text'], { stdio: 'pipe' });
      const accountId = accountResult.stdout.trim();
      
      // Create S3 backup bucket and credentials
      await createS3BackupBucket('us-east-1');
      await createS3CredentialsSecret(parsed.namespace, 'us-east-1');
      
      logInfo('Installing Percona operator...');
      await installOperator(parsed.namespace);
      
      // Wait for operator to be ready before installing cluster
      logInfo('Waiting for Percona operator to be ready...');
      await waitForOperatorReady(parsed.namespace);
      
      logInfo('Installing Percona cluster...');
      await installCluster(parsed.namespace, parsed.name, parsed.nodes, accountId);
      
      // Wait for cluster to be fully ready
      await waitForClusterReady(parsed.namespace, parsed.name, parsed.nodes);
      
      logSuccess('Percona operator and cluster installed and ready.');
    } else if (parsed.action === 'expand') {
      if (!parsed.size) {
        logError('Size parameter is required for expand command');
        process.exitCode = 1;
        return;
      }
      logInfo(`Expanding Percona cluster volumes to ${parsed.size}...`);
      await expandVolumes(parsed.namespace, parsed.name, parsed.size);
      logSuccess('Volume expansion completed.');
    } else {
      await uninstall(parsed.namespace, parsed.name);
      logSuccess('Uninstall completed.');
    }
  } catch (err) {
    logError('Percona script failed', err);
    process.exitCode = 1;
  }
}

main();


