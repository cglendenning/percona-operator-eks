Scenario,Primary Recovery Method,Alternate / Fallback,Detection (Signals),RTO (Target),RPO (Target),MTTR (Expected),Expected Data Loss,Likelihood,Business Impact,Affected Components,Notes / Assumptions
Single MySQL pod failure (container crash / OOM),K8s restarts pod; Percona Operator re‑joins PXC node automatically,Manual pod delete; ensure liveness/readiness probes healthy,Pod CrashLoopBackOff; PXC node missing; HAProxy/ProxySQL health check fails,5–10 minutes,0 (no data loss),10–20 minutes,None (Galera sync),Medium,Low,"PXC pod, sidecars, service endpoints",Assumes 3+ node PXC; quorum maintained; no PVC corruption.
Kubernetes worker node failure (VM host crash),Pods rescheduled by K8s; PXC node re‑joins cluster,Cordon/drain failing node; replace VM; verify anti‑affinity rules,Node NotReady; pod evictions; HAProxy backend down,10–20 minutes,0,30–60 minutes,None,Medium,Medium,"VMware host, kubelet, PXC pod on failed node",PodDisruptionBudgets and topology spread configured; PVCs on shared storage or local PVs with replicas.
Storage PVC corruption for a single PXC node,Remove failed node; recreate pod; let PXC SST/IST re‑seed from peers,Restore individual table/DB from S3 physical backup to side instance and logical import,InnoDB corruption; fsck errors; PXC node unable to start or desyncs,1–3 hours,0–5 minutes (IST),2–6 hours,"None to seconds (if IST fails then SST, still no loss)",Low,Medium,PersistentVolume for a pod,Cluster remains quorate; IST preferred; SST uses donor—ensure donor capacity.
Percona Operator / CRD misconfiguration (bad rollout),Rollback GitOps change in Rancher/Fleet; restore previous CR YAML,Scale down/up operator; recreate PXC from last good statefulset spec,Pods stuck Pending/CrashLoop; operator reconciliation errors,15–45 minutes,0,30–90 minutes,None,Medium,Medium,"Percona Operator, PXC CR, StatefulSets",All changes flow via Fleet (reviewed/approved); backup of last known good manifests.
Cluster loses quorum (multiple PXC pods down),Recover majority; bootstrap from most advanced node; follow Percona PXC bootstrap runbook,Promote secondary DC replica to primary; redirect traffic,Galera wsrep_cluster_status=non-Primary; no writes; 500s at app layer,30–90 minutes,0–60 seconds,1–3 hours,None to <1 minute (unflushed tx),Low,High,"Multiple PXC pods, operator, HAProxy/ProxySQL",Requires careful bootstrap—pick the node with highest seqno; verify app read/write mode.
Primary DC network partition from Secondary (WAN cut),Stay primary in current DC; queue async replication; monitor lag,"If app tier in secondary is hot‑standby, execute app failover if primary DC instability persists",Replication IO thread error; ping loss; WAN monitoring alerts,0 (no failover by default),N/A (stays primary),30–120 minutes (provider repair),None (no role change),Medium,Low → Medium (risk if prolonged),"WAN, routers, firewalls, replication channel",Percona Replication is asynchronous; avoid split‑brain by not auto‑failing over.
Primary DC power/cooling outage (site down),Promote Secondary DC replica to primary (planned role swap),Restore latest backup to Secondary if replica is stale/unhealthy,Site monitors red; all nodes unreachable; out‑of‑band alerts,30–120 minutes,30–120 seconds (typical async lag),"2–6 hours (site), but service restored on secondary in ≤2 hours",Up to replication lag at failover (seconds → minutes),Low,Critical,"Entire primary K8s, storage, network",Runbooks and DNS/ingress switch prepared; writes paused during role change; app config points to new VIP/ingress.
Both DCs up but replication stops (broken channel),Fix replication (purge relay logs; CHANGE MASTER to correct coordinates; GTID resync),"If diverged, rebuild replica from S3 backup + binlogs",Seconds_Behind_Master increasing; IO/SQL thread stopped,15–60 minutes,0 (no failover),30–120 minutes,None (still primary),Medium,Medium (DR posture degraded),"MySQL replication channel, binlogs, network",Ensure binlog retention ≥ rebuild time; monitor for errant transactions.
Accidental DROP/DELETE/TRUNCATE (logical data loss),Point‑in‑time restore from S3 backup + binlogs to side instance; recover affected tables via mysqlpump/mydumper,"If using Percona Backup for MySQL (PBM physical), do tablespace‑level restore where possible",App errors; missing rows; audit logs; sudden size drops,1–4 hours (depends on dataset size),≤ 5 minutes,2–8 hours,Up to RPO (5–15 minutes typical),Medium,High,Data layer; backups; binlogs,Frequent binlog backups to S3; tested PITR runbooks; separate restore host available.
Widespread data corruption (bad migration/script),PITR to pre‑change timestamp on clean environment; validate; cutover,"If change is reversible, apply compensating migration from audit trail",Integrity checks fail; anomaly detection; app incidents post‑deploy,2–6 hours,5–15 minutes,4–12 hours,Minutes (to chosen restore point),Low,High,"DB schema/data, CI/CD change",Strict change windows; mandatory backup OK prior to migrations.
S3 backup target unavailable (regional outage or ACL/cred issue),Buffer locally; failover to secondary S3 bucket; rotate IAM credentials,Temporarily write backups to secondary DC object store/NAS,PBM/xtrabackup errors; 5xx from S3; IAM access denied,0 (no runtime failover),N/A (runtime unaffected),1–3 hours,Risk only if outage spans retention window,Medium,Medium (restores at risk),"Backup jobs, S3 bucket, IAM keys",Cross‑region/bucket replication enabled; periodic restore tests prove viability.
Backups complete but are non‑restorable (silent failure),Detect via scheduled restore drills; fix pipeline; re‑run full backup,Use previous verified backup then roll forward via binlogs,Automated restore test failures; checksum mismatches,Restore drill ≤ 4 hours,≤ 15 minutes,4–12 hours (to rebuild trust),Up to RPO (if incident),Low,High,"Backup artifacts, metadata, scripts",Keep last N verified points; store manifests/checksums with backups.
Kubernetes control plane outage (API server down),Restore control plane VMs; failover etcd; use Rancher to re‑provision,Operate cluster as‑is (pods keep running); avoid changes until API is back,kubectl timeouts; Rancher unhealthy; etcd alarms,30–90 minutes,0,1–3 hours,None,Low,Medium,"etcd, API server, controllers",App continues if no scaling needed; ensure etcd backups exist and tested.
Ransomware on VMware hosts (storage encrypted),Isolate; rebuild hosts; restore K8s and PXC from clean backups in secondary DC,Failover to Secondary DC replica; rebuild primary later,Crypto activity; EDR alerts; sudden file access errors,2–8 hours (service via secondary),30–120 seconds (replication lag),1–3 days (full infra rebuild),Seconds → minutes (at failover),Low,Critical,"VMware hosts, storage, K8s nodes, DB",Immutable backups; off‑site copies; tested DC failover runbooks.
Credential compromise (DB or S3 keys),Rotate credentials; revoke sessions; rotate S3/IAM; audit access,"If suspected data tamper, execute PITR to clean point",Anomalous access; GuardDuty/SIEM; IAM alerts,30–120 minutes,0–15 minutes (if PITR needed),2–8 hours,Potential rollback of recent writes if PITR,Medium,High,"DB users, IAM, CI/CD secrets",Secret rotation via Fleet; least privilege enforced; MFA on admins.
Ingress/VIP failure (HAProxy/ProxySQL service unreachable),Fail traffic to alternate service/ingress; fix Service/Endpoints,Clients connect via read/write split endpoints directly,Health checks fail; 502/503; service endpoints empty,10–30 minutes,0,30–60 minutes,None,Medium,High (app down though DB healthy),"K8s Service, HAProxy/ProxySQL, DNS/ingress",Dual ingress paths; Service monitors; out‑of‑band jump path.