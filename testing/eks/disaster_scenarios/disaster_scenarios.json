{
  "scenarios": [
    {
      "scenario": "Single MySQL pod failure (container crash / OOM)",
    "primary_recovery_method": "K8s restarts pod; Percona Operator re\u2011joins PXC node automatically",
    "alternate_fallback": "Manual pod delete; ensure liveness/readiness probes healthy",
    "detection_signals": "Pod CrashLoopBackOff; PXC node missing; HAProxy/ProxySQL health check fails",
    "rto_target": "10 minutes",
    "rpo_target": "0 (no data loss)",
    "mttr_expected": "10\u201320 minutes",
    "expected_data_loss": "None (Galera sync)",
    "likelihood": "medium",
    "business_impact": "low",
    "affected_components": "PXC pod, sidecars, service endpoints",
    "notes_assumptions": "Assumes 3+ node PXC; quorum maintained; no PVC corruption.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/component=pxc",
    "app_kind": "statefulset",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 600,
    "poll_interval": 15,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete a single PXC pod and verify cluster recovers",
    "test_file": "test_dr_single_mysql_pod_failure.py"
  },
  {
    "scenario": "Kubernetes worker node failure (VM host crash)",
    "primary_recovery_method": "Pods rescheduled by K8s; PXC node re\u2011joins cluster",
    "alternate_fallback": "Cordon/drain failing node; replace VM; verify anti\u2011affinity rules",
    "detection_signals": "Node NotReady; pod evictions; HAProxy backend down",
    "rto_target": "20 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201360 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "VMware host, kubelet, PXC pod on failed node",
    "notes_assumptions": "PodDisruptionBudgets and topology spread configured; PVCs on shared storage or local PVs with replicas.",
    "test_enabled": true,
    "chaos_type": "node-drain",
    "target_label": "app.kubernetes.io/component=pxc",
    "app_kind": "statefulset",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 1200,
    "poll_interval": 30,
    "total_chaos_duration": 300,
    "chaos_interval": 60,
    "test_description": "Drain a node hosting PXC pods and verify rescheduling and recovery",
    "test_file": "test_dr_kubernetes_worker_node_failure.py"
  },
  {
    "scenario": "Storage PVC corruption for a single PXC node",
    "primary_recovery_method": "Remove failed node; recreate pod; let PXC SST/IST re\u2011seed from peers",
    "alternate_fallback": "Restore individual table/DB from S3 physical backup to side instance and logical import",
    "detection_signals": "InnoDB corruption; fsck errors; PXC node unable to start or desyncs",
    "rto_target": "3 hours",
    "rpo_target": "5 minutes",
    "mttr_expected": "2\u20136 hours",
    "expected_data_loss": "None to seconds (if IST fails then SST, still no loss)",
    "likelihood": "low",
    "business_impact": "medium",
    "affected_components": "PersistentVolume for a pod",
    "notes_assumptions": "Cluster remains quorate; IST preferred; SST uses donor\u2014ensure donor capacity.",
    "test_enabled": false,
    "test_description": "PVC corruption requires destructive disk operations not suitable for automated testing",
    "test_file": null
  },
  {
    "scenario": "Percona Operator / CRD misconfiguration (bad rollout)",
    "primary_recovery_method": "Rollback GitOps change in Rancher/Fleet; restore previous CR YAML",
    "alternate_fallback": "Scale down/up operator; recreate PXC from last good statefulset spec",
    "detection_signals": "Pods stuck Pending/CrashLoop; operator reconciliation errors",
    "rto_target": "45 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201390 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "Percona Operator, PXC CR, StatefulSets",
    "notes_assumptions": "All changes flow via Fleet (reviewed/approved); backup of last known good manifests.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/name=percona-xtradb-cluster-operator",
    "app_kind": "deployment",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 900,
    "poll_interval": 30,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete operator pod and verify it recovers and reconciles cluster",
    "test_file": "test_dr_percona_operator_crd_misconfiguration.py"
  },
  {
    "scenario": "Schema change or DDL blocks writes",
    "primary_recovery_method": "Kill blocking DDL process if safe; wait for completion if near end; rollback DDL if possible",
    "alternate_fallback": "If DDL cannot be killed safely, failover to replica; restore from backup if DDL corrupted schema",
    "detection_signals": "Writes blocked; 'Waiting for table metadata lock' errors; DDL process running long; application timeouts on writes",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "15-60 minutes",
    "expected_data_loss": "None if handled correctly; potential data loss if DDL is killed mid-operation",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to long-running DDL operations (ALTER TABLE on large tables, CREATE INDEX, etc.), unexpected DDL operations during peak hours, or DDL operations that deadlock with application transactions. More common with large tables or when DDL is not properly planned.",
    "business_impact": "high",
    "affected_components": "Database schema, table locks, write operations, application transactions",
    "notes_assumptions": "DDL operations are monitored; ability to identify and kill DDL processes safely; replica available for failover if needed; backup available if schema corruption occurs.",
    "test_enabled": true,
    "test_description": "Test creates table, inserts data, starts uncommitted transaction, runs DDL to create blocking, then verifies killing DDL unblocks writes",
    "test_file": "test_dr_schema_change_or_ddl_blocks_writes.py"
  },
  {
    "scenario": "Cluster loses quorum (multiple PXC pods down)",
    "primary_recovery_method": "Recover majority; bootstrap from most advanced node; follow Percona PXC bootstrap runbook",
    "alternate_fallback": "Promote secondary DC replica to primary; redirect traffic",
    "detection_signals": "Galera wsrep_cluster_status=non-Primary; no writes; 500s at app layer",
    "rto_target": "90 minutes",
    "rpo_target": "60 seconds",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "None to <1 minute (unflushed tx)",
    "likelihood": "low",
    "business_impact": "high",
    "affected_components": "Multiple PXC pods, operator, HAProxy/ProxySQL",
    "notes_assumptions": "Requires careful bootstrap\u2014pick the node with highest seqno; verify app read/write mode.",
    "test_enabled": false,
    "test_description": "Quorum loss requires careful manual bootstrap procedures not suitable for automated testing",
    "test_file": null
  },
  {
    "scenario": "Primary DC network partition from Secondary (WAN cut)",
    "primary_recovery_method": "Stay primary in current DC; queue async replication; monitor lag",
    "alternate_fallback": "If app tier in secondary is hot\u2011standby, execute app failover if primary DC instability persists",
    "detection_signals": "Replication IO thread error; ping loss; WAN monitoring alerts",
    "rto_target": "0 (no failover by default)",
    "rpo_target": "N/A (stays primary)",
    "mttr_expected": "30\u2013120 minutes (provider repair)",
    "expected_data_loss": "None (no role change)",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "WAN, routers, firewalls, replication channel",
    "notes_assumptions": "Percona Replication is asynchronous; avoid split\u2011brain by not auto\u2011failing over.",
    "test_enabled": false,
    "test_description": "Multi-DC WAN partition requires infrastructure beyond single cluster scope",
    "test_file": null
  },
  {
    "scenario": "Primary DC power/cooling outage (site down)",
    "primary_recovery_method": "Promote Secondary DC replica to primary (planned role swap)",
    "alternate_fallback": "Restore latest backup to Secondary if replica is stale/unhealthy",
    "detection_signals": "Site monitors red; all nodes unreachable; out\u2011of\u2011band alerts",
    "rto_target": "120 minutes",
    "rpo_target": "120 seconds",
    "mttr_expected": "2\u20136 hours (site), but service restored on secondary in \u22642 hours",
    "expected_data_loss": "Up to replication lag at failover (seconds \u2192 minutes)",
    "likelihood": "low",
    "likelihood_justification": "Low probability due to redundant power/cooling systems, UPS, and generator backups at modern datacenters. However, regional events (storms, earthquakes) or extended utility outages can occur. Consider regional risk factors.",
    "business_impact": "critical",
    "affected_components": "Entire primary K8s, storage, network",
    "notes_assumptions": "Runbooks and DNS/ingress switch prepared; writes paused during role change; app config points to new VIP/ingress.",
    "test_enabled": false,
    "test_description": "Full DC outage requires multi-DC infrastructure",
    "test_file": null
  },
  {
    "scenario": "Both DCs up but replication stops (broken channel)",
    "primary_recovery_method": "Fix replication (purge relay logs; CHANGE MASTER to correct coordinates; GTID resync)",
    "alternate_fallback": "If diverged, rebuild replica from S3 backup + binlogs",
    "detection_signals": "Seconds_Behind_Master increasing; IO/SQL thread stopped",
    "rto_target": "60 minutes",
    "rpo_target": "0 (no failover)",
    "mttr_expected": "30\u2013120 minutes",
    "expected_data_loss": "None (still primary)",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "MySQL replication channel, binlogs, network",
    "notes_assumptions": "Ensure binlog retention \u2265 rebuild time; monitor for errant transactions.",
    "test_enabled": false,
    "test_description": "Multi-DC replication requires secondary DC infrastructure",
    "test_file": null
  },
  {
    "scenario": "Accidental DROP/DELETE/TRUNCATE (logical data loss)",
    "primary_recovery_method": "Point\u2011in\u2011time restore from S3 backup + binlogs to side instance; recover affected tables via mysqlpump/mydumper",
    "alternate_fallback": "If using Percona Backup for MySQL (PBM physical), do tablespace\u2011level restore where possible",
    "detection_signals": "App errors; missing rows; audit logs; sudden size drops",
    "rto_target": "4 hours",
    "rpo_target": "5 minutes",
    "mttr_expected": "2\u20138 hours",
    "expected_data_loss": "Up to RPO (5\u201315 minutes typical)",
    "likelihood": "medium",
    "business_impact": "high",
    "affected_components": "Data layer; backups; binlogs",
    "notes_assumptions": "Frequent binlog backups to S3; tested PITR runbooks; separate restore host available.",
    "test_enabled": false,
    "test_description": "PITR testing requires extensive setup with test data and validation queries",
    "test_file": null
  },
  {
    "scenario": "Widespread data corruption (bad migration/script)",
    "primary_recovery_method": "PITR to pre\u2011change timestamp on clean environment; validate; cutover",
    "alternate_fallback": "If change is reversible, apply compensating migration from audit trail",
    "detection_signals": "Integrity checks fail; anomaly detection; app incidents post\u2011deploy",
    "rto_target": "6 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4\u201312 hours",
    "expected_data_loss": "Minutes (to chosen restore point)",
    "likelihood": "low",
    "business_impact": "high",
    "affected_components": "DB schema/data, CI/CD change",
    "notes_assumptions": "Strict change windows; mandatory backup OK prior to migrations.",
    "test_enabled": false,
    "test_description": "Data corruption scenarios require complex data validation logic",
    "test_file": null
  },
  {
    "scenario": "S3 backup target unavailable (regional outage or ACL/cred issue)",
    "primary_recovery_method": "Buffer locally; failover to secondary S3 bucket; rotate IAM credentials",
    "alternate_fallback": "Temporarily write backups to secondary DC object store/NAS",
    "detection_signals": "PBM/xtrabackup errors; 5xx from S3; IAM access denied",
    "rto_target": "0 (no runtime failover)",
    "rpo_target": "N/A (runtime unaffected)",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "Risk only if outage spans retention window",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "Backup jobs, S3 bucket, IAM keys",
    "notes_assumptions": "Cross\u2011region/bucket replication enabled; periodic restore tests prove viability.",
    "test_enabled": false,
    "test_description": "S3 outage simulation requires infrastructure access and credential manipulation",
    "test_file": null
  },
  {
    "scenario": "Backups complete but are non\u2011restorable (silent failure)",
    "primary_recovery_method": "Detect via scheduled restore drills; fix pipeline; re\u2011run full backup",
    "alternate_fallback": "Use previous verified backup then roll forward via binlogs",
    "detection_signals": "Automated restore test failures; checksum mismatches",
    "rto_target": "4 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4\u201312 hours (to rebuild trust)",
    "expected_data_loss": "Up to RPO (if incident)",
    "likelihood": "low",
    "likelihood_justification": "Low but non-zero. Silent failures can occur due to network interruptions during upload, disk space issues, version mismatches, or corrupted source data. Automated restore testing should catch most cases, but edge cases may slip through.",
    "business_impact": "high",
    "affected_components": "Backup artifacts, metadata, scripts",
    "notes_assumptions": "Keep last N verified points; store manifests/checksums with backups.",
    "test_enabled": false,
    "test_description": "Backup validation is covered by separate backup/restore integration tests",
    "test_file": null
  },
  {
    "scenario": "Kubernetes control plane outage (API server down)",
    "primary_recovery_method": "Restore control plane VMs; failover etcd; use Rancher to re\u2011provision",
    "alternate_fallback": "Operate cluster as\u2011is (pods keep running); avoid changes until API is back",
    "detection_signals": "kubectl timeouts; Rancher unhealthy; etcd alarms",
    "rto_target": "90 minutes",
    "rpo_target": "0",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "None",
    "likelihood": "low",
    "business_impact": "medium",
    "affected_components": "etcd, API server, controllers",
    "notes_assumptions": "App continues if no scaling needed; ensure etcd backups exist and tested.",
    "test_enabled": false,
    "test_description": "Control plane testing would disrupt test execution itself",
    "test_file": null
  },
  {
    "scenario": "Ransomware Attack",
    "primary_recovery_method": "Isolate; rebuild hosts; restore K8s and PXC from clean backups in secondary DC",
    "alternate_fallback": "Failover to Secondary DC replica; rebuild primary later",
    "detection_signals": "Crypto activity; EDR alerts; sudden file access errors",
    "rto_target": "8 hours",
    "rpo_target": "120 seconds",
    "mttr_expected": "1\u20133 days (full infra rebuild)",
    "expected_data_loss": "Seconds \u2192 minutes (at failover)",
    "likelihood": "low",
    "likelihood_justification": "Low but increasing risk. Based on industry trends, ransomware attacks on infrastructure have increased. Likelihood depends on security posture, network segmentation, and EDR effectiveness. Consider industry vertical risk (healthcare, finance higher risk).",
    "business_impact": "critical",
    "affected_components": "EC2 instances, EBS volumes, K8s nodes, DB",
    "notes_assumptions": "Immutable backups; off\u2011site copies; tested DC failover runbooks.",
    "test_enabled": false,
    "test_description": "Ransomware simulation requires AWS/storage layer access",
    "test_file": null
  },
  {
    "scenario": "Credential compromise (DB or S3 keys)",
    "primary_recovery_method": "Rotate credentials; revoke sessions; rotate S3/IAM; audit access",
    "alternate_fallback": "If suspected data tamper, execute PITR to clean point",
    "detection_signals": "Anomalous access; GuardDuty/SIEM; IAM alerts",
    "rto_target": "120 minutes",
    "rpo_target": "15 minutes",
    "mttr_expected": "2\u20138 hours",
    "expected_data_loss": "Potential rollback of recent writes if PITR",
    "likelihood": "medium",
    "business_impact": "high",
    "affected_components": "DB users, IAM, CI/CD secrets",
    "notes_assumptions": "Secret rotation via Fleet; least privilege enforced; MFA on admins.",
    "test_enabled": false,
    "test_description": "Credential testing requires access to secret management systems",
    "test_file": null
  },
  {
    "scenario": "HAProxy endpoints inaccessible",
    "primary_recovery_method": "Fix K8s Service/Endpoints configuration; restore ingress/DNS routing; verify network connectivity",
    "alternate_fallback": "Clients connect via read/write split endpoints directly to PXC (bypass HAProxy)",
    "detection_signals": "Health checks fail; 502/503 errors; service endpoints empty; HAProxy pods healthy but unreachable from applications",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201360 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "business_impact": "high",
    "affected_components": "K8s Service endpoints, DNS/ingress routing, network connectivity (HAProxy pods remain healthy)",
    "notes_assumptions": "HAProxy pods are running and healthy; issue is with Service endpoints, ingress, DNS, or network routing preventing application access.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/component=proxysql",
    "app_kind": "statefulset",
    "expected_recovery": "service_endpoints",
    "mttr_seconds": 600,
    "poll_interval": 15,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete ProxySQL pod and verify service endpoints recover",
    "test_file": "test_dr_ingressvip_failure.py"
  },
  {
    "scenario": "Database disk space exhaustion (binlog/undo log full)",
    "primary_recovery_method": "Free space by purging old binlogs; increase EBS volume size; enable binlog rotation",
    "alternate_fallback": "Temporarily disable binlog; restore from backup after space freed",
    "detection_signals": "Disk usage alerts; 'No space left on device' errors; binlog rotation failures; writes failing",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "30-60 minutes",
    "expected_data_loss": "None if caught early; potential loss if writes blocked",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to growth in transaction volume, binlog retention settings, or monitoring gaps. More common in high-transaction environments or when retention policies are misconfigured.",
    "business_impact": "high",
    "affected_components": "PXC pods, binlog storage, undo logs, temporary tables",
    "notes_assumptions": "Monitoring alerts configured; binlog retention policies set; EBS volume expansion procedures documented.",
    "test_enabled": false,
    "test_description": "Disk exhaustion testing requires filling storage which may impact other tests",
    "test_file": null
  },
  {
    "scenario": "Connection pool exhaustion (max_connections reached)",
    "primary_recovery_method": "Kill idle/long-running connections; increase max_connections; identify connection leaks",
    "alternate_fallback": "Restart PXC pods to reset connection count; fix application connection pooling",
    "detection_signals": "Connection refused errors; 'Too many connections' errors; application timeouts; max_connections metric at limit",
    "rto_target": "15 minutes",
    "rpo_target": "0",
    "mttr_expected": "15-30 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood, especially with connection leak bugs, misconfigured connection pools, or sudden traffic spikes. More common during application deployments or when connection limits are set too low.",
    "business_impact": "high",
    "affected_components": "Database connections, application connection pools, HAProxy/ProxySQL",
    "notes_assumptions": "Connection monitoring in place; application connection pooling configured correctly; max_connections tuned appropriately.",
    "test_enabled": false,
    "test_description": "Connection exhaustion testing requires simulating many connections which may impact cluster stability",
    "test_file": null
  },
  {
    "scenario": "Sustained high load causing performance degradation",
    "primary_recovery_method": "Scale up resources; optimize slow queries; add read replicas; implement query throttling",
    "alternate_fallback": "Enable read-only mode temporarily; failover to secondary DC if available",
    "detection_signals": "High CPU/memory usage; slow query log alerts; increased response times; connection timeouts",
    "rto_target": "60 minutes",
    "rpo_target": "0",
    "mttr_expected": "1-3 hours",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to traffic spikes, inefficient queries, missing indexes, or resource constraints. More common during peak usage periods or after schema changes.",
    "business_impact": "high",
    "affected_components": "CPU, memory, I/O, query execution, replication lag",
    "notes_assumptions": "Performance monitoring and alerting configured; capacity planning in place; query optimization procedures documented.",
    "test_enabled": false,
    "test_description": "Performance degradation testing requires sustained load generation which may impact test environment",
    "test_file": null
  },
  {
    "scenario": "S3 service failure (backup target unavailable)",
    "primary_recovery_method": "Failover to secondary S3 bucket/region; buffer backups locally; check AWS service health",
    "alternate_fallback": "Temporarily write backups to EBS volumes; restore S3 access when available",
    "detection_signals": "S3 5xx errors; backup job failures; AWS service health dashboard alerts; IAM access denied",
    "rto_target": "0 (no runtime failover)",
    "rpo_target": "N/A (runtime unaffected)",
    "mttr_expected": "30-90 minutes",
    "expected_data_loss": "Risk only if outage spans retention window",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to AWS regional outages, IAM credential issues, or bucket misconfiguration. AWS S3 has high availability but regional outages do occur. More common with single-region deployments.",
    "business_impact": "medium",
    "affected_components": "S3 buckets, backup jobs, IAM roles",
    "notes_assumptions": "Cross-region S3 replication enabled; secondary S3 bucket available; local backup buffering possible.",
    "test_enabled": false,
    "test_description": "S3 failure simulation requires AWS infrastructure access",
    "test_file": null
  },
  {
    "scenario": "Audit log corruption or loss (compliance violation)",
    "primary_recovery_method": "Restore audit logs from backup; regenerate from binlogs if possible; document gap for auditors",
    "alternate_fallback": "Reconstruct audit trail from application logs; implement compensating controls",
    "detection_signals": "Audit log file corruption; missing audit entries; integrity check failures; compliance monitoring alerts",
    "rto_target": "2 hours",
    "rpo_target": "0",
    "mttr_expected": "2-8 hours",
    "expected_data_loss": "Audit trail gaps (compliance risk)",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood due to audit log protection mechanisms, but can occur due to storage corruption, EBS failures, or misconfiguration. Critical for SOX, ISO 27001, and SOC 2 compliance.",
    "business_impact": "low",
    "affected_components": "Audit logs, compliance reporting, regulatory requirements",
    "notes_assumptions": "Audit logs backed up separately; integrity checks enabled; compliance team notified immediately.",
    "test_enabled": false,
    "test_description": "Audit log corruption testing requires careful handling to avoid actual compliance violations",
    "test_file": null
  },
  {
    "scenario": "Backup retention policy failure (backups deleted prematurely)",
    "primary_recovery_method": "Restore from remaining backups; implement retention policy fixes; verify backup lifecycle",
    "alternate_fallback": "Recover from secondary DC backups; restore from off-site archives if available",
    "detection_signals": "Backup count below expected; S3 lifecycle policy misconfiguration; automated deletion alerts",
    "rto_target": "4 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4-12 hours",
    "expected_data_loss": "Up to RPO if incident occurs during gap",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood but high impact. Can occur due to misconfigured S3 lifecycle policies, script errors, or manual deletion. Critical for compliance with retention requirements (SOX, GDPR, etc.).",
    "business_impact": "high",
    "affected_components": "Backup retention policies, S3 bucket lifecycle, compliance requirements",
    "notes_assumptions": "Backup retention policies documented and tested; monitoring for backup count; S3 object lock enabled where required.",
    "test_enabled": false,
    "test_description": "Backup deletion testing risks actual data loss",
    "test_file": null
  }
  ],
  "discarded_scenarios": [
    {
      "scenario": "Secondary DC failure during primary DC recovery",
      "reason": "No recovery process documentation created - scenario represents cascading failure requiring complex multi-DC coordination beyond standard recovery procedures"
    },
    {
      "scenario": "Multiple infrastructure failures occurring simultaneously",
      "reason": "No recovery process documentation created - scenario represents worst-case cascading failure scenario requiring complex multi-system coordination and incident response beyond standard database recovery procedures. Recovery depends on specific combination of failures and requires comprehensive incident management rather than a single recovery playbook."
    }
  ]
}
