{
  "scenarios": [
    {
      "scenario": "Single MySQL pod failure (container crash / OOM)",
    "primary_recovery_method": "K8s restarts pod; Percona Operator re\u2011joins PXC node automatically",
    "alternate_fallback": "Manual pod delete; ensure liveness/readiness probes healthy",
    "detection_signals": "Pod CrashLoopBackOff; PXC node missing; HAProxy/ProxySQL health check fails",
    "rto_target": "10 minutes",
    "rpo_target": "0 (no data loss)",
    "mttr_expected": "10\u201320 minutes",
    "expected_data_loss": "None (Galera sync)",
    "likelihood": "medium",
    "business_impact": "low",
    "affected_components": "PXC pod, sidecars, service endpoints",
    "notes_assumptions": "Assumes 3+ node PXC; quorum maintained; no PVC corruption.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/component=pxc",
    "app_kind": "statefulset",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 600,
    "poll_interval": 15,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete a single PXC pod and verify cluster recovers",
    "test_file": "test_dr_single_mysql_pod_failure.py"
  },
  {
    "scenario": "Kubernetes worker node failure (VM host crash)",
    "primary_recovery_method": "Pods rescheduled by K8s; PXC node re\u2011joins cluster",
    "alternate_fallback": "Cordon/drain failing node; replace VM; verify anti\u2011affinity rules",
    "detection_signals": "Node NotReady; pod evictions; HAProxy backend down",
    "rto_target": "20 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201360 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "VMware host, kubelet, PXC pod on failed node",
    "notes_assumptions": "PodDisruptionBudgets and topology spread configured; PVCs on shared storage or local PVs with replicas.",
    "test_enabled": true,
    "chaos_type": "node-drain",
    "target_label": "app.kubernetes.io/component=pxc",
    "app_kind": "statefulset",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 1200,
    "poll_interval": 30,
    "total_chaos_duration": 300,
    "chaos_interval": 60,
    "test_description": "Drain a node hosting PXC pods and verify rescheduling and recovery",
    "test_file": "test_dr_kubernetes_worker_node_failure.py"
  },
  {
    "scenario": "Storage PVC corruption for a single PXC node",
    "primary_recovery_method": "Remove failed node; recreate pod; let PXC SST/IST re\u2011seed from peers",
    "alternate_fallback": "Restore individual table/DB from S3 physical backup to side instance and logical import",
    "detection_signals": "InnoDB corruption; fsck errors; PXC node unable to start or desyncs",
    "rto_target": "3 hours",
    "rpo_target": "5 minutes",
    "mttr_expected": "2\u20136 hours",
    "expected_data_loss": "None to seconds (if IST fails then SST, still no loss)",
    "likelihood": "low",
    "business_impact": "medium",
    "affected_components": "PersistentVolume for a pod",
    "notes_assumptions": "Cluster remains quorate; IST preferred; SST uses donor\u2014ensure donor capacity.",
    "test_enabled": false,
    "test_description": "PVC corruption requires destructive disk operations not suitable for automated testing",
    "test_file": null
  },
  {
    "scenario": "Percona Operator / CRD misconfiguration (bad rollout)",
    "primary_recovery_method": "Rollback GitOps change in Rancher/Fleet; restore previous CR YAML",
    "alternate_fallback": "Scale down/up operator; recreate PXC from last good statefulset spec",
    "detection_signals": "Pods stuck Pending/CrashLoop; operator reconciliation errors",
    "rto_target": "45 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201390 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "Percona Operator, PXC CR, StatefulSets",
    "notes_assumptions": "All changes flow via Fleet (reviewed/approved); backup of last known good manifests.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/name=percona-xtradb-cluster-operator",
    "app_kind": "deployment",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 900,
    "poll_interval": 30,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete operator pod and verify it recovers and reconciles cluster",
    "test_file": "test_dr_percona_operator_crd_misconfiguration.py"
  },
  {
    "scenario": "Schema change or DDL blocks writes",
    "primary_recovery_method": "Kill blocking DDL process if safe; wait for completion if near end; rollback DDL if possible",
    "alternate_fallback": "If DDL cannot be killed safely, failover to replica; restore from backup if DDL corrupted schema",
    "detection_signals": "Writes blocked; 'Waiting for table metadata lock' errors; DDL process running long; application timeouts on writes",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "15-60 minutes",
    "expected_data_loss": "None if handled correctly; potential data loss if DDL is killed mid-operation",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to long-running DDL operations (ALTER TABLE on large tables, CREATE INDEX, etc.), unexpected DDL operations during peak hours, or DDL operations that deadlock with application transactions. More common with large tables or when DDL is not properly planned.",
    "business_impact": "high",
    "affected_components": "Database schema, table locks, write operations, application transactions",
    "notes_assumptions": "DDL operations are monitored; ability to identify and kill DDL processes safely; replica available for failover if needed; backup available if schema corruption occurs.",
    "test_enabled": true,
    "test_description": "Test creates table, inserts data, starts uncommitted transaction, runs DDL to create blocking, then verifies killing DDL unblocks writes",
    "test_file": "test_dr_schema_change_or_ddl_blocks_writes.py"
  },
  {
    "scenario": "Cluster loses quorum (multiple PXC pods down)",
    "primary_recovery_method": "Recover majority; bootstrap from most advanced node; follow Percona PXC bootstrap runbook",
    "alternate_fallback": "Promote secondary DC replica to primary; redirect traffic",
    "detection_signals": "Galera wsrep_cluster_status=non-Primary; no writes; 500s at app layer",
    "rto_target": "90 minutes",
    "rpo_target": "60 seconds",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "None to <1 minute (unflushed tx)",
    "likelihood": "low",
    "business_impact": "high",
    "affected_components": "Multiple PXC pods, operator, HAProxy/ProxySQL",
    "notes_assumptions": "Requires careful bootstrap\u2014pick the node with highest seqno; verify app read/write mode.",
    "test_enabled": false,
    "test_description": "Quorum loss requires careful manual bootstrap procedures not suitable for automated testing",
    "test_file": null
  },
  {
    "scenario": "Primary DC network partition from Secondary (WAN cut)",
    "primary_recovery_method": "Stay primary in current DC; queue async replication; monitor lag",
    "alternate_fallback": "If app tier in secondary is hot\u2011standby, execute app failover if primary DC instability persists",
    "detection_signals": "Replication IO thread error; ping loss; WAN monitoring alerts",
    "rto_target": "0 (no failover by default)",
    "rpo_target": "N/A (stays primary)",
    "mttr_expected": "30\u2013120 minutes (provider repair)",
    "expected_data_loss": "None (no role change)",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "WAN, routers, firewalls, replication channel",
    "notes_assumptions": "Percona Replication is asynchronous; avoid split\u2011brain by not auto\u2011failing over.",
    "test_enabled": false,
    "test_description": "Multi-DC WAN partition requires infrastructure beyond single cluster scope",
    "test_file": null
  },
  {
    "scenario": "Primary DC power/cooling outage (site down)",
    "primary_recovery_method": "Promote Secondary DC replica to primary (planned role swap)",
    "alternate_fallback": "Restore latest backup to Secondary if replica is stale/unhealthy",
    "detection_signals": "Site monitors red; all nodes unreachable; out\u2011of\u2011band alerts",
    "rto_target": "30 minutes",
    "rpo_target": "60 seconds behind",
    "mttr_expected": "3 days",
    "expected_data_loss": "60 seconds",
    "likelihood": "low",
    "likelihood_justification": "Low probability due to redundant power/cooling systems, UPS, and generator backups at modern datacenters. However, regional events (storms, earthquakes) or extended utility outages can occur. Consider regional risk factors.",
    "business_impact": "critical",
    "affected_components": "Entire primary K8s, storage, network",
    "notes_assumptions": "Runbooks and DNS/ingress switch prepared; writes paused during role change; app config points to new VIP/ingress.",
    "test_enabled": false,
    "test_description": "Full DC outage requires multi-DC infrastructure",
    "test_file": null
  },
  {
    "scenario": "Both DCs up but replication stops (broken channel)",
    "primary_recovery_method": "Fix replication (purge relay logs; CHANGE MASTER to correct coordinates; GTID resync)",
    "alternate_fallback": "If diverged, rebuild replica from S3 backup + binlogs",
    "detection_signals": "Seconds_Behind_Master increasing; IO/SQL thread stopped",
    "rto_target": "60 minutes",
    "rpo_target": "0 (no failover)",
    "mttr_expected": "30\u2013120 minutes",
    "expected_data_loss": "None (still primary)",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "MySQL replication channel, binlogs, network",
    "notes_assumptions": "Ensure binlog retention \u2265 rebuild time; monitor for errant transactions.",
    "test_enabled": false,
    "test_description": "Multi-DC replication requires secondary DC infrastructure",
    "test_file": null
  },
  {
    "scenario": "Accidental DROP/DELETE/TRUNCATE (logical data loss)",
    "primary_recovery_method": "Point\u2011in\u2011time restore from S3 backup + binlogs to side instance; recover affected tables via mysqlpump/mydumper",
    "alternate_fallback": "If using Percona Backup for MySQL (PBM physical), do tablespace\u2011level restore where possible",
    "detection_signals": "App errors; missing rows; audit logs; sudden size drops",
    "rto_target": "4 hours",
    "rpo_target": "5 minutes",
    "mttr_expected": "2\u20138 hours",
    "expected_data_loss": "Up to RPO (5\u201315 minutes typical)",
    "likelihood": "medium",
    "business_impact": "high",
    "affected_components": "Data layer; backups; binlogs",
    "notes_assumptions": "Frequent binlog backups to S3; tested PITR runbooks; separate restore host available.",
    "test_enabled": false,
    "test_description": "PITR testing requires extensive setup with test data and validation queries",
    "test_file": null
  },
  {
    "scenario": "Widespread data corruption (bad migration/script)",
    "primary_recovery_method": "PITR to pre\u2011change timestamp on clean environment; validate; cutover",
    "alternate_fallback": "If change is reversible, apply compensating migration from audit trail",
    "detection_signals": "Integrity checks fail; anomaly detection; app incidents post\u2011deploy",
    "rto_target": "6 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4\u201312 hours",
    "expected_data_loss": "Minutes (to chosen restore point)",
    "likelihood": "low",
    "business_impact": "high",
    "affected_components": "DB schema/data, CI/CD change",
    "notes_assumptions": "Strict change windows; mandatory backup OK prior to migrations.",
    "test_enabled": false,
    "test_description": "Data corruption scenarios require complex data validation logic",
    "test_file": null
  },
  {
    "scenario": "S3 backup target unavailable (regional outage or ACL/cred issue)",
    "primary_recovery_method": "Buffer locally; failover to secondary S3 bucket; rotate IAM credentials",
    "alternate_fallback": "Temporarily write backups to secondary DC object store/NAS",
    "detection_signals": "PBM/xtrabackup errors; 5xx from S3; IAM access denied",
    "rto_target": "0 (no runtime failover)",
    "rpo_target": "N/A (runtime unaffected)",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "Risk only if outage spans retention window",
    "likelihood": "medium",
    "business_impact": "medium",
    "affected_components": "Backup jobs, S3 bucket, IAM keys",
    "notes_assumptions": "Cross\u2011region/bucket replication enabled; periodic restore tests prove viability.",
    "test_enabled": false,
    "test_description": "S3 outage simulation requires infrastructure access and credential manipulation",
    "test_file": null
  },
  {
    "scenario": "Backups complete but are non\u2011restorable (silent failure)",
    "primary_recovery_method": "Detect via scheduled restore drills; fix pipeline; re\u2011run full backup",
    "alternate_fallback": "Use previous verified backup then roll forward via binlogs",
    "detection_signals": "Automated restore test failures; checksum mismatches",
    "rto_target": "4 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4\u201312 hours (to rebuild trust)",
    "expected_data_loss": "Up to RPO (if incident)",
    "likelihood": "low",
    "likelihood_justification": "Low but non-zero. Silent failures can occur due to network interruptions during upload, disk space issues, version mismatches, or corrupted source data. Automated restore testing should catch most cases, but edge cases may slip through.",
    "business_impact": "high",
    "affected_components": "Backup artifacts, metadata, scripts",
    "notes_assumptions": "Keep last N verified points; store manifests/checksums with backups.",
    "test_enabled": false,
    "test_description": "Backup validation is covered by separate backup/restore integration tests",
    "test_file": null
  },
  {
    "scenario": "Kubernetes control plane outage (API server down)",
    "primary_recovery_method": "Restore control plane VMs; failover etcd; use Rancher to re\u2011provision",
    "alternate_fallback": "Operate cluster as\u2011is (pods keep running); avoid changes until API is back",
    "detection_signals": "kubectl timeouts; Rancher unhealthy; etcd alarms",
    "rto_target": "90 minutes",
    "rpo_target": "0",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "None",
    "likelihood": "low",
    "business_impact": "medium",
    "affected_components": "etcd, API server, controllers",
    "notes_assumptions": "App continues if no scaling needed; ensure etcd backups exist and tested.",
    "test_enabled": false,
    "test_description": "Control plane testing would disrupt test execution itself",
    "test_file": null
  },
  {
    "scenario": "Ransomware attack",
    "primary_recovery_method": "Isolate; rebuild hosts; restore K8s and PXC from clean backups in secondary DC",
    "alternate_fallback": "Failover to Secondary DC replica; rebuild primary later",
    "detection_signals": "Crypto activity; EDR alerts; sudden file access errors",
    "rto_target": "8 hours",
    "rpo_target": "120 seconds",
    "mttr_expected": "1\u20133 days (full infra rebuild)",
    "expected_data_loss": "Seconds \u2192 minutes (at failover)",
    "likelihood": "low",
    "likelihood_justification": "Low but increasing risk. Based on industry trends, ransomware attacks on infrastructure have increased. Likelihood depends on security posture, network segmentation, and EDR effectiveness. Consider industry vertical risk (healthcare, finance higher risk).",
    "business_impact": "critical",
    "affected_components": "EC2 instances, EBS volumes, K8s nodes, DB",
    "notes_assumptions": "Immutable backups; off\u2011site copies; tested DC failover runbooks.",
    "test_enabled": false,
    "test_description": "Ransomware simulation requires AWS/storage layer access",
    "test_file": null
  },
  {
    "scenario": "Credential compromise (DB or S3 keys)",
    "primary_recovery_method": "Rotate credentials; revoke sessions; rotate S3/IAM; audit access",
    "alternate_fallback": "If suspected data tamper, execute PITR to clean point",
    "detection_signals": "Anomalous access; GuardDuty/SIEM; IAM alerts",
    "rto_target": "120 minutes",
    "rpo_target": "15 minutes",
    "mttr_expected": "2\u20138 hours",
    "expected_data_loss": "Potential rollback of recent writes if PITR",
    "likelihood": "medium",
    "business_impact": "high",
    "affected_components": "DB users, IAM, CI/CD secrets",
    "notes_assumptions": "Secret rotation via Fleet; least privilege enforced; MFA on admins.",
    "test_enabled": false,
    "test_description": "Credential testing requires access to secret management systems",
    "test_file": null
  },
  {
    "scenario": "HAProxy endpoints inaccessible",
    "primary_recovery_method": "Fix K8s Service/Endpoints configuration; restore ingress/DNS routing; verify network connectivity",
    "alternate_fallback": "Clients connect via read/write split endpoints directly to PXC (bypass HAProxy)",
    "detection_signals": "Health checks fail; 502/503 errors; service endpoints empty; HAProxy pods healthy but unreachable from applications",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201360 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "business_impact": "high",
    "affected_components": "K8s Service endpoints, DNS/ingress routing, network connectivity (HAProxy pods remain healthy)",
    "notes_assumptions": "HAProxy pods are running and healthy; issue is with Service endpoints, ingress, DNS, or network routing preventing application access.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/component=proxysql",
    "app_kind": "statefulset",
    "expected_recovery": "service_endpoints",
    "mttr_seconds": 600,
    "poll_interval": 15,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete ProxySQL pod and verify service endpoints recover",
    "test_file": "test_dr_ingressvip_failure.py"
  },
  {
    "scenario": "Database disk space exhaustion (data directory)",
    "primary_recovery_method": "Identify space consumer (binlogs, data files, undo logs); purge old binlogs; enable log rotation; increase EBS volume size",
    "alternate_fallback": "Temporarily disable binlogging; restore from backup after space freed",
    "detection_signals": "Disk usage alerts; 'No space left on device' errors; write failures; binlog accumulation; undo log growth",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "30-60 minutes",
    "expected_data_loss": "None if caught early; potential loss if writes blocked",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to binlog retention, data growth, or long-running transactions causing undo log growth. Persistent storage issues that require manual intervention.",
    "business_impact": "high",
    "affected_components": "PXC pods, EBS volumes, binlogs, undo logs, redo logs, data files, InnoDB tablespaces",
    "notes_assumptions": "Monitoring alerts configured; log retention policies set; EBS volume expansion procedures documented. Distinct from temp space issues.",
    "test_enabled": false,
    "test_description": "Disk exhaustion testing requires filling storage which may impact other tests",
    "test_file": null
  },
  {
    "scenario": "Temporary tablespace exhaustion",
    "primary_recovery_method": "Identify and kill queries creating large temp tables; increase tmp_table_size/max_heap_table_size; add dedicated tmpdir volume",
    "alternate_fallback": "Restart MySQL to clear temp files; optimize queries to avoid temp table creation",
    "detection_signals": "Disk usage spike then sudden drop; 'No space left on device' during query execution; queries failing mid-execution; temp directory fills then empties",
    "rto_target": "15 minutes",
    "rpo_target": "0",
    "mttr_expected": "15-30 minutes",
    "expected_data_loss": "None (temp tables are transient)",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood, especially with complex queries, large sorts, GROUP BY operations, or joins without indexes. Often confusing because temp files are cleaned up when the query dies, leaving disk appearing free when you investigate.",
    "business_impact": "medium",
    "affected_components": "MySQL temp directory, query execution, sorts, joins, GROUP BY operations, derived tables",
    "notes_assumptions": "Temp directory may be on same volume as data directory or separate. Key indicator: disk alert fires but disk appears free when you log in (temp files auto-cleaned on query failure).",
    "test_enabled": false,
    "test_description": "Temp space exhaustion testing requires running queries that create large temp tables",
    "test_file": null
  },
  {
    "scenario": "Connection pool exhaustion (max_connections reached)",
    "primary_recovery_method": "Kill idle/long-running connections; increase max_connections; identify connection leaks",
    "alternate_fallback": "Restart PXC pods to reset connection count; fix application connection pooling",
    "detection_signals": "Connection refused errors; 'Too many connections' errors; application timeouts; max_connections metric at limit",
    "rto_target": "15 minutes",
    "rpo_target": "0",
    "mttr_expected": "15-30 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood, especially with connection leak bugs, misconfigured connection pools, or sudden traffic spikes. More common during application deployments or when connection limits are set too low.",
    "business_impact": "high",
    "affected_components": "Database connections, application connection pools, HAProxy/ProxySQL",
    "notes_assumptions": "Connection monitoring in place; application connection pooling configured correctly; max_connections tuned appropriately.",
    "test_enabled": false,
    "test_description": "Connection exhaustion testing requires simulating many connections which may impact cluster stability",
    "test_file": null
  },
  {
    "scenario": "Increased API call volume causes performance degradation",
    "primary_recovery_method": "Scale up cluster by increasing size key for PXC and/or HAProxy in PerconaXtraDBCluster CR; push changes to appropriate branch",
    "alternate_fallback": "If scaling reveals query or data model inefficiencies, optimize queries and add missing indexes; implement query throttling if needed",
    "detection_signals": "High CPU/memory usage; increased API response times; connection timeouts; slow query log alerts; application performance degradation",
    "rto_target": "60 minutes",
    "rpo_target": "0",
    "mttr_expected": "1-3 hours",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to increased API call volume from application growth, marketing campaigns, or external factors. More common during peak usage periods or when application usage patterns change.",
    "business_impact": "high",
    "affected_components": "CPU, memory, I/O, query execution, PXC cluster, HAProxy, application response times",
    "notes_assumptions": "Performance monitoring and alerting configured; GitOps workflow in place for cluster scaling; ability to modify PerconaXtraDBCluster CR size values; query optimization expertise available if needed.",
    "test_enabled": false,
    "test_description": "Performance degradation testing requires sustained load generation which may impact test environment",
    "test_file": null
  },
  {
    "scenario": "Application change causes performance degradation",
    "primary_recovery_method": "Identify problematic query/change; rollback application deployment; optimize query; add missing indexes; redeploy fixed version",
    "alternate_fallback": "Temporarily block problematic application endpoint; throttle application requests; scale up database resources",
    "detection_signals": "Slow query log alerts after deployment; increased response times; high CPU/memory usage; full table scans; application timeouts; performance metrics degradation",
    "rto_target": "45 minutes",
    "rpo_target": "0",
    "mttr_expected": "30-90 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to frequent application deployments, code changes introducing inefficient queries (full table scans, missing WHERE clauses, selecting unnecessary columns), or ORM-generated queries that are not optimized. More common when query performance testing is not part of CI/CD pipeline or when developers are not aware of database performance implications.",
    "business_impact": "high",
    "affected_components": "Application queries, database performance, CPU, memory, I/O, query execution plans",
    "notes_assumptions": "Performance monitoring and slow query logging enabled; ability to identify problematic queries; application rollback procedures documented; query optimization expertise available.",
    "test_enabled": false,
    "test_description": "Application performance degradation testing requires identifying specific query inefficiencies introduced by code changes",
    "test_file": null
  },
  {
    "scenario": "S3 service failure (backup target unavailable)",
    "primary_recovery_method": "Failover to secondary S3 bucket/region; buffer backups locally; check AWS service health",
    "alternate_fallback": "Temporarily write backups to EBS volumes; restore S3 access when available",
    "detection_signals": "S3 5xx errors; backup job failures; AWS service health dashboard alerts; IAM access denied",
    "rto_target": "0 (no runtime failover)",
    "rpo_target": "N/A (runtime unaffected)",
    "mttr_expected": "30-90 minutes",
    "expected_data_loss": "Risk only if outage spans retention window",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to AWS regional outages, IAM credential issues, or bucket misconfiguration. AWS S3 has high availability but regional outages do occur. More common with single-region deployments.",
    "business_impact": "medium",
    "affected_components": "S3 buckets, backup jobs, IAM roles",
    "notes_assumptions": "Cross-region S3 replication enabled; secondary S3 bucket available; local backup buffering possible.",
    "test_enabled": false,
    "test_description": "S3 failure simulation requires AWS infrastructure access",
    "test_file": null
  },
  {
    "scenario": "Audit log corruption or loss (compliance violation)",
    "primary_recovery_method": "Restore audit logs from backup; regenerate from binlogs if possible; document gap for auditors",
    "alternate_fallback": "Reconstruct audit trail from application logs; implement compensating controls",
    "detection_signals": "Audit log file corruption; missing audit entries; integrity check failures; compliance monitoring alerts",
    "rto_target": "2 hours",
    "rpo_target": "0",
    "mttr_expected": "2-8 hours",
    "expected_data_loss": "Audit trail gaps (compliance risk)",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood due to audit log protection mechanisms, but can occur due to storage corruption, EBS failures, or misconfiguration. Critical for SOX, ISO 27001, and SOC 2 compliance.",
    "business_impact": "low",
    "affected_components": "Audit logs, compliance reporting, regulatory requirements",
    "notes_assumptions": "Audit logs backed up separately; integrity checks enabled; compliance team notified immediately.",
    "test_enabled": false,
    "test_description": "Audit log corruption testing requires careful handling to avoid actual compliance violations",
    "test_file": null
  },
  {
    "scenario": "Backup retention policy failure (backups deleted prematurely)",
    "primary_recovery_method": "Restore from remaining backups; implement retention policy fixes; verify backup lifecycle",
    "alternate_fallback": "Recover from secondary DC backups; restore from off-site archives if available",
    "detection_signals": "Backup count below expected; S3 lifecycle policy misconfiguration; automated deletion alerts",
    "rto_target": "4 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4-12 hours",
    "expected_data_loss": "Up to RPO if incident occurs during gap",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood but high impact. Can occur due to misconfigured S3 lifecycle policies, script errors, or manual deletion. Critical for compliance with retention requirements (SOX, GDPR, etc.).",
    "business_impact": "low",
    "affected_components": "Backup retention policies, S3 bucket lifecycle, compliance requirements",
    "notes_assumptions": "Backup retention policies documented and tested; monitoring for backup count; S3 object lock enabled where required.",
    "test_enabled": false,
    "test_description": "Backup deletion testing risks actual data loss",
    "test_file": null
  },
  {
    "scenario": "DNS resolution failure (internal or external)",
    "primary_recovery_method": "Fix DNS server/configuration; update /etc/hosts as temporary workaround; restore Route53/Cloud DNS service",
    "alternate_fallback": "Use IP addresses directly; update application connection strings; restore DNS when available",
    "detection_signals": "Connection timeouts; 'Name or service not known' errors; DNS query failures; application cannot resolve database hostnames",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "30-60 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to Route53/Cloud DNS failures, misconfiguration, VPC networking issues, or DNS cache poisoning. More common in complex AWS environments or during infrastructure changes.",
    "business_impact": "high",
    "affected_components": "Route53/Cloud DNS, DNS configuration, application connectivity, service discovery",
    "notes_assumptions": "DNS monitoring in place; ability to identify DNS server issues; /etc/hosts workaround available; DNS service recovery procedures documented.",
    "test_enabled": false,
    "test_description": "DNS failure simulation requires infrastructure access and may impact other services",
    "test_file": null
  },
  {
    "scenario": "Certificate expiration or revocation causing connection failures",
    "primary_recovery_method": "Renew/rotate certificates via AWS Certificate Manager or cert-manager; update Kubernetes secrets; restart pods to load new certificates",
    "alternate_fallback": "Temporarily disable certificate validation (development only); restore from certificate backup; use alternate certificate authority",
    "detection_signals": "SSL/TLS handshake failures; 'certificate expired' errors; 'certificate verify failed' errors; connection refused with SSL errors",
    "rto_target": "45 minutes",
    "rpo_target": "0",
    "mttr_expected": "30-90 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to certificate lifecycle management gaps, missed renewal reminders, or automated renewal failures. More common with short-lived certificates or complex certificate chains.",
    "business_impact": "high",
    "affected_components": "SSL/TLS certificates, AWS Certificate Manager, Kubernetes secrets, application connections, database connections",
    "notes_assumptions": "Certificate monitoring and alerting configured; certificate renewal procedures documented; certificate backup available; ability to quickly rotate certificates.",
    "test_enabled": false,
    "test_description": "Certificate expiration testing requires careful handling to avoid actual service disruption",
    "test_file": null
  },
  {
    "scenario": "Memory exhaustion causing OOM kills (out of memory)",
    "primary_recovery_method": "Identify memory leak; kill memory-intensive queries/processes; increase memory limits; restart affected pods",
    "alternate_fallback": "Scale up EKS node groups; enable swap temporarily; failover to secondary DC if available",
    "detection_signals": "OOM kill events in logs; 'Out of memory' errors; pod restarts; memory usage at 100%; CloudWatch memory alerts",
    "rto_target": "20 minutes",
    "rpo_target": "0",
    "mttr_expected": "20-60 minutes",
    "expected_data_loss": "None if handled quickly; potential loss if OOM kills cause data corruption",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to memory leaks, inefficient queries, buffer pool misconfiguration, or sudden traffic spikes. More common with large datasets or complex queries.",
    "business_impact": "high",
    "affected_components": "PXC pods, memory resources, query execution, buffer pools, EKS node resources",
    "notes_assumptions": "Memory monitoring and alerting configured; CloudWatch alarms set; ability to identify memory-intensive processes; memory limits properly configured; swap disabled for database workloads.",
    "test_enabled": false,
    "test_description": "Memory exhaustion testing requires careful resource management to avoid impacting other services",
    "test_file": null
  },
  {
    "scenario": "Clock skew between cluster nodes causing replication issues",
    "primary_recovery_method": "Synchronize NTP; restart chronyd/ntpd service; correct system time on affected nodes; verify time synchronization",
    "alternate_fallback": "Manually set system time if NTP unavailable; restart affected pods; rebuild replication if time drift is severe",
    "detection_signals": "Replication lag despite low load; timestamp inconsistencies; 'clock skew' warnings in logs; replication errors related to timestamps",
    "rto_target": "60 minutes",
    "rpo_target": "0",
    "mttr_expected": "30-120 minutes",
    "expected_data_loss": "None if corrected quickly; potential data inconsistency if time drift is severe",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood due to NTP synchronization, but can occur due to NTP server failures, EC2 instance clock drift, or network issues. More common in EKS when NTP is misconfigured or during instance migrations.",
    "business_impact": "high",
    "affected_components": "System clocks, NTP service, replication, timestamp-based operations, EC2 instance clocks",
    "notes_assumptions": "NTP monitoring configured; NTP servers available and reliable; ability to identify clock skew; time synchronization procedures documented.",
    "test_enabled": false,
    "test_description": "Clock skew testing requires careful handling to avoid actual time synchronization issues",
    "test_file": null
  },
  {
    "scenario": "Accidental production restore from wrong backup or wrong point in time",
    "primary_recovery_method": "Immediately stop restore if in progress; identify correct backup/point in time; restore from correct backup; validate data integrity",
    "alternate_fallback": "If restore completed, identify data loss scope; restore from correct backup; replay transactions from binlogs if available",
    "detection_signals": "Unexpected data state; missing recent data; application errors; audit logs showing restore operations; data timestamp mismatches",
    "rto_target": "4 hours",
    "rpo_target": "15 minutes",
    "mttr_expected": "4-12 hours",
    "expected_data_loss": "Up to RPO (15 minutes to hours depending on detection time)",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood but critical impact. Can occur due to human error, mislabeled backups, wrong environment selection, or automation failures. More common during high-stress incidents or when restore procedures are not well-documented.",
    "business_impact": "critical",
    "affected_components": "Production data, backup selection, restore procedures, data integrity, S3 backup artifacts",
    "notes_assumptions": "Backup labeling and verification procedures in place; restore approval workflows; ability to quickly identify incorrect restore; correct backup available in S3.",
    "test_enabled": false,
    "test_description": "Accidental restore testing risks actual data loss in production",
    "test_file": null
  },
  {
    "scenario": "Network policy misconfiguration blocking database access",
    "primary_recovery_method": "Identify and fix network policy rules; update NetworkPolicy resources; verify pod-to-pod connectivity",
    "alternate_fallback": "Temporarily remove restrictive network policies; use service mesh bypass if available; restore from network policy backup",
    "detection_signals": "Connection refused errors; network policy deny logs; pods cannot communicate; application cannot reach database",
    "rto_target": "30 minutes",
    "rpo_target": "0",
    "mttr_expected": "20-60 minutes",
    "expected_data_loss": "None",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to network policy changes, misconfigured rules, or policy updates during deployments. More common in environments with strict network segmentation or frequent policy updates.",
    "business_impact": "high",
    "affected_components": "Kubernetes NetworkPolicy, pod networking, service mesh, application connectivity, VPC security groups",
    "notes_assumptions": "Network policy monitoring in place; ability to identify blocking policies; network policy backup available; documented network policy procedures.",
    "test_enabled": false,
    "test_description": "Network policy testing requires careful handling to avoid actual network disruption",
    "test_file": null
  },
  {
    "scenario": "Application causing excessive replication lag",
    "primary_recovery_method": "Identify slow queries or bulk operations; optimize or throttle application queries; add read replicas; scale replication resources",
    "alternate_fallback": "Temporarily block problematic application; enable read-only mode on replica; accept increased RPO if lag is acceptable",
    "detection_signals": "Replication lag increasing; Seconds_Behind_Master growing; replica falling behind; application queries identified as cause; RPO exceeded on secondary site",
    "rto_target": "4 hours",
    "rpo_target": "0",
    "mttr_expected": "2-8 hours",
    "expected_data_loss": "None (primary unaffected); RPO exceeded on secondary site (compliance/DR readiness impact)",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to inefficient application queries, bulk operations, missing indexes, or application bugs causing replication bottlenecks. More common during predictable peak hours or data migrations. Primary cluster operation is unaffected.",
    "business_impact": "medium",
    "affected_components": "Replication threads, application queries, replica performance, binlog processing, secondary DC RPO compliance",
    "notes_assumptions": "Replication lag monitoring configured; ability to identify problematic queries; query throttling mechanisms available; read replica scaling possible; primary cluster continues normal operation.",
    "test_enabled": false,
    "test_description": "Replication lag testing requires careful load generation to avoid impacting production",
    "test_file": null
  },
  {
    "scenario": "Monitoring and alerting system failure during incident",
    "primary_recovery_method": "Restore monitoring services; use alternative monitoring tools; rely on manual checks and kubectl commands",
    "alternate_fallback": "Use basic system commands (top, df, netstat); check application logs directly; use backup monitoring systems if available",
    "detection_signals": "Monitoring dashboards unavailable; alerting system down; metrics collection failures; no visibility into system state",
    "rto_target": "N/A (monitoring failure does not affect database)",
    "rpo_target": "N/A",
    "mttr_expected": "30-120 minutes",
    "expected_data_loss": "None (monitoring failure does not cause data loss)",
    "likelihood": "medium",
    "likelihood_justification": "Medium likelihood due to monitoring system failures, network issues, or resource exhaustion in monitoring infrastructure. More common when monitoring systems share infrastructure with production or during large-scale incidents.",
    "business_impact": "medium",
    "affected_components": "Monitoring systems, alerting, metrics collection, observability, CloudWatch",
    "notes_assumptions": "Manual diagnostic procedures documented; alternative monitoring tools available; ability to operate without monitoring; monitoring system recovery procedures in place.",
    "test_enabled": false,
    "test_description": "Monitoring failure testing requires careful handling to avoid impacting actual monitoring",
    "test_file": null
  },
  {
    "scenario": "Encryption key rotation failure (database or backup encryption)",
    "primary_recovery_method": "Rollback key rotation; restore previous key from AWS KMS; fix key rotation process; retry rotation after validation",
    "alternate_fallback": "Use backup encryption keys; restore from unencrypted backup if available; re-encrypt data with new keys",
    "detection_signals": "Encryption/decryption errors; key rotation job failures; database unable to read encrypted data; backup restore failures; AWS KMS errors",
    "rto_target": "90 minutes",
    "rpo_target": "0",
    "mttr_expected": "1-4 hours",
    "expected_data_loss": "None if handled correctly; potential data loss if keys are lost",
    "likelihood": "low",
    "likelihood_justification": "Low likelihood but high impact. Can occur due to AWS KMS failures, misconfigured rotation procedures, or key storage issues. Critical for compliance with encryption requirements (PCI DSS, HIPAA, etc.).",
    "business_impact": "high",
    "affected_components": "Encryption keys, AWS KMS, encrypted data, backup encryption, EBS encryption",
    "notes_assumptions": "Key rotation procedures documented and tested; key backup available; ability to rollback key rotation; AWS KMS monitoring in place.",
    "test_enabled": false,
    "test_description": "Encryption key rotation testing requires careful handling to avoid actual key loss",
    "test_file": null
  }
  ],
  "discarded_scenarios": [
    {
      "scenario": "Secondary DC failure during primary DC recovery",
      "reason": "No recovery process documentation created - scenario represents cascading failure requiring complex multi-DC coordination beyond standard recovery procedures"
    },
    {
      "scenario": "Multiple infrastructure failures occurring simultaneously",
      "reason": "No recovery process documentation created - scenario represents worst-case cascading failure scenario requiring complex multi-system coordination and incident response beyond standard database recovery procedures. Recovery depends on specific combination of failures and requires comprehensive incident management rather than a single recovery playbook."
    }
  ]
}
