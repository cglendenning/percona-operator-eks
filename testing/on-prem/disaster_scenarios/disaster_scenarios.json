[
  {
    "scenario": "Single MySQL pod failure (container crash / OOM)",
    "primary_recovery_method": "K8s restarts pod; Percona Operator re\u2011joins PXC node automatically",
    "alternate_fallback": "Manual pod delete; ensure liveness/readiness probes healthy",
    "detection_signals": "Pod CrashLoopBackOff; PXC node missing; HAProxy/ProxySQL health check fails",
    "rto_target": "5\u201310 minutes",
    "rpo_target": "0 (no data loss)",
    "mttr_expected": "10\u201320 minutes",
    "expected_data_loss": "None (Galera sync)",
    "likelihood": "Medium",
    "business_impact": "Low",
    "affected_components": "PXC pod, sidecars, service endpoints",
    "notes_assumptions": "Assumes 3+ node PXC; quorum maintained; no PVC corruption.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/component=pxc",
    "app_kind": "statefulset",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 600,
    "poll_interval": 15,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete a single PXC pod and verify cluster recovers",
    "test_file": "test_dr_single_mysql_pod_failure.py"
  },
  {
    "scenario": "Kubernetes worker node failure (VM host crash)",
    "primary_recovery_method": "Pods rescheduled by K8s; PXC node re\u2011joins cluster",
    "alternate_fallback": "Cordon/drain failing node; replace VM; verify anti\u2011affinity rules",
    "detection_signals": "Node NotReady; pod evictions; HAProxy backend down",
    "rto_target": "10\u201320 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201360 minutes",
    "expected_data_loss": "None",
    "likelihood": "Medium",
    "business_impact": "Medium",
    "affected_components": "VMware host, kubelet, PXC pod on failed node",
    "notes_assumptions": "PodDisruptionBudgets and topology spread configured; PVCs on shared storage or local PVs with replicas.",
    "test_enabled": true,
    "chaos_type": "node-drain",
    "target_label": "app.kubernetes.io/component=pxc",
    "app_kind": "statefulset",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 1200,
    "poll_interval": 30,
    "total_chaos_duration": 300,
    "chaos_interval": 60,
    "test_description": "Drain a node hosting PXC pods and verify rescheduling and recovery",
    "test_file": "test_dr_kubernetes_worker_node_failure.py"
  },
  {
    "scenario": "Storage PVC corruption for a single PXC node",
    "primary_recovery_method": "Remove failed node; recreate pod; let PXC SST/IST re\u2011seed from peers",
    "alternate_fallback": "Restore individual table/DB from S3 physical backup to side instance and logical import",
    "detection_signals": "InnoDB corruption; fsck errors; PXC node unable to start or desyncs",
    "rto_target": "1\u20133 hours",
    "rpo_target": "0\u20135 minutes (IST)",
    "mttr_expected": "2\u20136 hours",
    "expected_data_loss": "None to seconds (if IST fails then SST, still no loss)",
    "likelihood": "Low",
    "business_impact": "Medium",
    "affected_components": "PersistentVolume for a pod",
    "notes_assumptions": "Cluster remains quorate; IST preferred; SST uses donor\u2014ensure donor capacity.",
    "test_enabled": false,
    "test_description": "PVC corruption requires destructive disk operations not suitable for automated testing",
    "test_file": null
  },
  {
    "scenario": "Percona Operator / CRD misconfiguration (bad rollout)",
    "primary_recovery_method": "Rollback GitOps change in Rancher/Fleet; restore previous CR YAML",
    "alternate_fallback": "Scale down/up operator; recreate PXC from last good statefulset spec",
    "detection_signals": "Pods stuck Pending/CrashLoop; operator reconciliation errors",
    "rto_target": "15\u201345 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201390 minutes",
    "expected_data_loss": "None",
    "likelihood": "Medium",
    "business_impact": "Medium",
    "affected_components": "Percona Operator, PXC CR, StatefulSets",
    "notes_assumptions": "All changes flow via Fleet (reviewed/approved); backup of last known good manifests.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/name=percona-xtradb-cluster-operator",
    "app_kind": "deployment",
    "expected_recovery": "cluster_ready",
    "mttr_seconds": 900,
    "poll_interval": 30,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete operator pod and verify it recovers and reconciles cluster",
    "test_file": "test_dr_percona_operator_crd_misconfiguration.py"
  },
  {
    "scenario": "Cluster loses quorum (multiple PXC pods down)",
    "primary_recovery_method": "Recover majority; bootstrap from most advanced node; follow Percona PXC bootstrap runbook",
    "alternate_fallback": "Promote secondary DC replica to primary; redirect traffic",
    "detection_signals": "Galera wsrep_cluster_status=non-Primary; no writes; 500s at app layer",
    "rto_target": "30\u201390 minutes",
    "rpo_target": "0\u201360 seconds",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "None to <1 minute (unflushed tx)",
    "likelihood": "Low",
    "business_impact": "High",
    "affected_components": "Multiple PXC pods, operator, HAProxy/ProxySQL",
    "notes_assumptions": "Requires careful bootstrap\u2014pick the node with highest seqno; verify app read/write mode.",
    "test_enabled": false,
    "test_description": "Quorum loss requires careful manual bootstrap procedures not suitable for automated testing",
    "test_file": null
  },
  {
    "scenario": "Primary DC network partition from Secondary (WAN cut)",
    "primary_recovery_method": "Stay primary in current DC; queue async replication; monitor lag",
    "alternate_fallback": "If app tier in secondary is hot\u2011standby, execute app failover if primary DC instability persists",
    "detection_signals": "Replication IO thread error; ping loss; WAN monitoring alerts",
    "rto_target": "0 (no failover by default)",
    "rpo_target": "N/A (stays primary)",
    "mttr_expected": "30\u2013120 minutes (provider repair)",
    "expected_data_loss": "None (no role change)",
    "likelihood": "Medium",
    "business_impact": "Low \u2192 Medium (risk if prolonged)",
    "affected_components": "WAN, routers, firewalls, replication channel",
    "notes_assumptions": "Percona Replication is asynchronous; avoid split\u2011brain by not auto\u2011failing over.",
    "test_enabled": false,
    "test_description": "Multi-DC WAN partition requires infrastructure beyond single cluster scope",
    "test_file": null
  },
  {
    "scenario": "Primary DC power/cooling outage (site down)",
    "primary_recovery_method": "Promote Secondary DC replica to primary (planned role swap)",
    "alternate_fallback": "Restore latest backup to Secondary if replica is stale/unhealthy",
    "detection_signals": "Site monitors red; all nodes unreachable; out\u2011of\u2011band alerts",
    "rto_target": "30\u2013120 minutes",
    "rpo_target": "30\u2013120 seconds (typical async lag)",
    "mttr_expected": "2\u20136 hours (site), but service restored on secondary in \u22642 hours",
    "expected_data_loss": "Up to replication lag at failover (seconds \u2192 minutes)",
    "likelihood": "Low",
    "business_impact": "Critical",
    "affected_components": "Entire primary K8s, storage, network",
    "notes_assumptions": "Runbooks and DNS/ingress switch prepared; writes paused during role change; app config points to new VIP/ingress.",
    "test_enabled": false,
    "test_description": "Full DC outage requires multi-DC infrastructure",
    "test_file": null
  },
  {
    "scenario": "Both DCs up but replication stops (broken channel)",
    "primary_recovery_method": "Fix replication (purge relay logs; CHANGE MASTER to correct coordinates; GTID resync)",
    "alternate_fallback": "If diverged, rebuild replica from S3 backup + binlogs",
    "detection_signals": "Seconds_Behind_Master increasing; IO/SQL thread stopped",
    "rto_target": "15\u201360 minutes",
    "rpo_target": "0 (no failover)",
    "mttr_expected": "30\u2013120 minutes",
    "expected_data_loss": "None (still primary)",
    "likelihood": "Medium",
    "business_impact": "Medium (DR posture degraded)",
    "affected_components": "MySQL replication channel, binlogs, network",
    "notes_assumptions": "Ensure binlog retention \u2265 rebuild time; monitor for errant transactions.",
    "test_enabled": false,
    "test_description": "Multi-DC replication requires secondary DC infrastructure",
    "test_file": null
  },
  {
    "scenario": "Accidental DROP/DELETE/TRUNCATE (logical data loss)",
    "primary_recovery_method": "Point\u2011in\u2011time restore from S3 backup + binlogs to side instance; recover affected tables via mysqlpump/mydumper",
    "alternate_fallback": "If using Percona Backup for MySQL (PBM physical), do tablespace\u2011level restore where possible",
    "detection_signals": "App errors; missing rows; audit logs; sudden size drops",
    "rto_target": "1\u20134 hours (depends on dataset size)",
    "rpo_target": "\u2264 5 minutes",
    "mttr_expected": "2\u20138 hours",
    "expected_data_loss": "Up to RPO (5\u201315 minutes typical)",
    "likelihood": "Medium",
    "business_impact": "High",
    "affected_components": "Data layer; backups; binlogs",
    "notes_assumptions": "Frequent binlog backups to S3; tested PITR runbooks; separate restore host available.",
    "test_enabled": false,
    "test_description": "PITR testing requires extensive setup with test data and validation queries",
    "test_file": null
  },
  {
    "scenario": "Widespread data corruption (bad migration/script)",
    "primary_recovery_method": "PITR to pre\u2011change timestamp on clean environment; validate; cutover",
    "alternate_fallback": "If change is reversible, apply compensating migration from audit trail",
    "detection_signals": "Integrity checks fail; anomaly detection; app incidents post\u2011deploy",
    "rto_target": "2\u20136 hours",
    "rpo_target": "5\u201315 minutes",
    "mttr_expected": "4\u201312 hours",
    "expected_data_loss": "Minutes (to chosen restore point)",
    "likelihood": "Low",
    "business_impact": "High",
    "affected_components": "DB schema/data, CI/CD change",
    "notes_assumptions": "Strict change windows; mandatory backup OK prior to migrations.",
    "test_enabled": false,
    "test_description": "Data corruption scenarios require complex data validation logic",
    "test_file": null
  },
  {
    "scenario": "S3 backup target unavailable (regional outage or ACL/cred issue)",
    "primary_recovery_method": "Buffer locally; failover to secondary S3 bucket; rotate IAM credentials",
    "alternate_fallback": "Temporarily write backups to secondary DC object store/NAS",
    "detection_signals": "PBM/xtrabackup errors; 5xx from S3; IAM access denied",
    "rto_target": "0 (no runtime failover)",
    "rpo_target": "N/A (runtime unaffected)",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "Risk only if outage spans retention window",
    "likelihood": "Medium",
    "business_impact": "Medium (restores at risk)",
    "affected_components": "Backup jobs, S3 bucket, IAM keys",
    "notes_assumptions": "Cross\u2011region/bucket replication enabled; periodic restore tests prove viability.",
    "test_enabled": false,
    "test_description": "S3 outage simulation requires infrastructure access and credential manipulation",
    "test_file": null
  },
  {
    "scenario": "Backups complete but are non\u2011restorable (silent failure)",
    "primary_recovery_method": "Detect via scheduled restore drills; fix pipeline; re\u2011run full backup",
    "alternate_fallback": "Use previous verified backup then roll forward via binlogs",
    "detection_signals": "Automated restore test failures; checksum mismatches",
    "rto_target": "Restore drill \u2264 4 hours",
    "rpo_target": "\u2264 15 minutes",
    "mttr_expected": "4\u201312 hours (to rebuild trust)",
    "expected_data_loss": "Up to RPO (if incident)",
    "likelihood": "Low",
    "business_impact": "High",
    "affected_components": "Backup artifacts, metadata, scripts",
    "notes_assumptions": "Keep last N verified points; store manifests/checksums with backups.",
    "test_enabled": false,
    "test_description": "Backup validation is covered by separate backup/restore integration tests",
    "test_file": null
  },
  {
    "scenario": "Kubernetes control plane outage (API server down)",
    "primary_recovery_method": "Restore control plane VMs; failover etcd; use Rancher to re\u2011provision",
    "alternate_fallback": "Operate cluster as\u2011is (pods keep running); avoid changes until API is back",
    "detection_signals": "kubectl timeouts; Rancher unhealthy; etcd alarms",
    "rto_target": "30\u201390 minutes",
    "rpo_target": "0",
    "mttr_expected": "1\u20133 hours",
    "expected_data_loss": "None",
    "likelihood": "Low",
    "business_impact": "Medium",
    "affected_components": "etcd, API server, controllers",
    "notes_assumptions": "App continues if no scaling needed; ensure etcd backups exist and tested.",
    "test_enabled": false,
    "test_description": "Control plane testing would disrupt test execution itself",
    "test_file": null
  },
  {
    "scenario": "Ransomware on VMware hosts (storage encrypted)",
    "primary_recovery_method": "Isolate; rebuild hosts; restore K8s and PXC from clean backups in secondary DC",
    "alternate_fallback": "Failover to Secondary DC replica; rebuild primary later",
    "detection_signals": "Crypto activity; EDR alerts; sudden file access errors",
    "rto_target": "2\u20138 hours (service via secondary)",
    "rpo_target": "30\u2013120 seconds (replication lag)",
    "mttr_expected": "1\u20133 days (full infra rebuild)",
    "expected_data_loss": "Seconds \u2192 minutes (at failover)",
    "likelihood": "Low",
    "business_impact": "Critical",
    "affected_components": "VMware hosts, storage, K8s nodes, DB",
    "notes_assumptions": "Immutable backups; off\u2011site copies; tested DC failover runbooks.",
    "test_enabled": false,
    "test_description": "Ransomware simulation requires VMware/storage layer access",
    "test_file": null
  },
  {
    "scenario": "Credential compromise (DB or S3 keys)",
    "primary_recovery_method": "Rotate credentials; revoke sessions; rotate S3/IAM; audit access",
    "alternate_fallback": "If suspected data tamper, execute PITR to clean point",
    "detection_signals": "Anomalous access; GuardDuty/SIEM; IAM alerts",
    "rto_target": "30\u2013120 minutes",
    "rpo_target": "0\u201315 minutes (if PITR needed)",
    "mttr_expected": "2\u20138 hours",
    "expected_data_loss": "Potential rollback of recent writes if PITR",
    "likelihood": "Medium",
    "business_impact": "High",
    "affected_components": "DB users, IAM, CI/CD secrets",
    "notes_assumptions": "Secret rotation via Fleet; least privilege enforced; MFA on admins.",
    "test_enabled": false,
    "test_description": "Credential testing requires access to secret management systems",
    "test_file": null
  },
  {
    "scenario": "Ingress/VIP failure (HAProxy/ProxySQL service unreachable)",
    "primary_recovery_method": "Fail traffic to alternate service/ingress; fix Service/Endpoints",
    "alternate_fallback": "Clients connect via read/write split endpoints directly",
    "detection_signals": "Health checks fail; 502/503; service endpoints empty",
    "rto_target": "10\u201330 minutes",
    "rpo_target": "0",
    "mttr_expected": "30\u201360 minutes",
    "expected_data_loss": "None",
    "likelihood": "Medium",
    "business_impact": "High (app down though DB healthy)",
    "affected_components": "K8s Service, HAProxy/ProxySQL, DNS/ingress",
    "notes_assumptions": "Dual ingress paths; Service monitors; out\u2011of\u2011band jump path.",
    "test_enabled": true,
    "chaos_type": "pod-delete",
    "target_label": "app.kubernetes.io/component=proxysql",
    "app_kind": "statefulset",
    "expected_recovery": "service_endpoints",
    "mttr_seconds": 600,
    "poll_interval": 15,
    "total_chaos_duration": 60,
    "chaos_interval": 10,
    "test_description": "Delete ProxySQL pod and verify service endpoints recover",
    "test_file": "test_dr_ingressvip_failure.py"
  }
]
